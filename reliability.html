<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 The Reliability of Measurement | The Twisted Pear</title>
  <meta name="description" content="The learning materials for Ability Asssessment." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 The Reliability of Measurement | The Twisted Pear" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The learning materials for Ability Asssessment." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 The Reliability of Measurement | The Twisted Pear" />
  
  <meta name="twitter:description" content="The learning materials for Ability Asssessment." />
  

<meta name="author" content="Kevin Rowley" />


<meta name="date" content="2025-02-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="NDSM.html"/>
<link rel="next" href="validity.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Assessment of Psychological Attributes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> General Features of Psychological Tests</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#types-of-psychological-test"><i class="fa fa-check"></i><b>2.1</b> Types of Psychological Test</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#specific-features-of-each-type-of-test"><i class="fa fa-check"></i><b>2.1.1</b> Specific Features of each Type of Test</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#using-tests-for-occupational-purposes"><i class="fa fa-check"></i><b>2.2</b> Using Tests for Occupational Purposes</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#good-practice-for-psychological-testing"><i class="fa fa-check"></i><b>2.2.1</b> Good Practice for Psychological Testing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#the-usefulness-of-tests-in-selection-for-jobs"><i class="fa fa-check"></i><b>2.3</b> The Usefulness of Tests in Selection for Jobs</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#validity-generalization-and-meta-analysis"><i class="fa fa-check"></i><b>2.4</b> Validity Generalization and Meta-analysis</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#implications-from-validity-generalization"><i class="fa fa-check"></i><b>2.5</b> Implications from Validity Generalization</a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#determinants-of-job-performance"><i class="fa fa-check"></i><b>2.6</b> Determinants of Job Performance</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#what-is-meant-by-general-mental-ability-gma"><i class="fa fa-check"></i><b>2.7</b> What is meant by General Mental Ability (GMA)</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#criterion-related-validity-coefficients-and-the-coefficient-of-determination"><i class="fa fa-check"></i><b>2.8</b> Criterion-related Validity Coefficients and the Coefficient of Determination</a></li>
<li class="chapter" data-level="2.9" data-path="intro.html"><a href="intro.html#criterion-related-validity-and-multiple-correlation"><i class="fa fa-check"></i><b>2.9</b> Criterion-Related Validity and Multiple-Correlation</a></li>
<li class="chapter" data-level="2.10" data-path="intro.html"><a href="intro.html#indicative-general-references-on-selection"><i class="fa fa-check"></i><b>2.10</b> Indicative General References on Selection</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="NDSM.html"><a href="NDSM.html"><i class="fa fa-check"></i><b>3</b> The Normal Distribution and scales of Measurement</a>
<ul>
<li class="chapter" data-level="3.1" data-path="NDSM.html"><a href="NDSM.html#standard-scores-z"><i class="fa fa-check"></i><b>3.1</b> Standard scores (z)</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="NDSM.html"><a href="NDSM.html#normalising-z-scores"><i class="fa fa-check"></i><b>3.1.1</b> Normalising z scores</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="NDSM.html"><a href="NDSM.html#standardised-scores"><i class="fa fa-check"></i><b>3.2</b> Standardised scores</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reliability.html"><a href="reliability.html"><i class="fa fa-check"></i><b>4</b> The Reliability of Measurement</a>
<ul>
<li class="chapter" data-level="4.1" data-path="reliability.html"><a href="reliability.html#reliability-1"><i class="fa fa-check"></i><b>4.1</b> Reliability</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="reliability.html"><a href="reliability.html#few-items-will-not-represent-the-whole-domain-of-the-construct"><i class="fa fa-check"></i><b>4.1.1</b> Few items will not represent the whole domain of the construct</a></li>
<li class="chapter" data-level="4.1.2" data-path="reliability.html"><a href="reliability.html#difficulty-levels-cannot-be-properly-varied-with-few-items-and-so-cannot-reveal-a-range-a-difference-between-people."><i class="fa fa-check"></i><b>4.1.2</b> Difficulty levels cannot be properly varied with few items and so cannot reveal a range a difference between people.</a></li>
<li class="chapter" data-level="4.1.3" data-path="reliability.html"><a href="reliability.html#any-one-item-may-be-poorly-constructed"><i class="fa fa-check"></i><b>4.1.3</b> Any one item may be poorly constructed</a></li>
<li class="chapter" data-level="4.1.4" data-path="reliability.html"><a href="reliability.html#any-one-item-may-offer-advantage-or-disadvantage-to-the-test-taker-because-of-their-previous-experience"><i class="fa fa-check"></i><b>4.1.4</b> Any one item may offer advantage or disadvantage to the test taker because of their previous experience</a></li>
<li class="chapter" data-level="4.1.5" data-path="reliability.html"><a href="reliability.html#any-one-item-may-be-affected-by-very-temporary-events"><i class="fa fa-check"></i><b>4.1.5</b> Any one item may be affected by very temporary events</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="reliability.html"><a href="reliability.html#reliability-and-classical-test-theory"><i class="fa fa-check"></i><b>4.2</b> Reliability and Classical Test Theory</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="reliability.html"><a href="reliability.html#determining-the-reliability-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Determining the Reliability Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="reliability.html"><a href="reliability.html#test-retest-methods"><i class="fa fa-check"></i><b>4.2.2</b> Test-Retest Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="reliability.html"><a href="reliability.html#alternate-form-methods-parallel-form-methods"><i class="fa fa-check"></i><b>4.2.3</b> Alternate Form Methods (Parallel form methods)</a></li>
<li class="chapter" data-level="4.2.4" data-path="reliability.html"><a href="reliability.html#split-half-methods"><i class="fa fa-check"></i><b>4.2.4</b> Split-half Methods</a></li>
<li class="chapter" data-level="4.2.5" data-path="reliability.html"><a href="reliability.html#internal-consistency-methods"><i class="fa fa-check"></i><b>4.2.5</b> Internal Consistency Methods</a></li>
<li class="chapter" data-level="4.2.6" data-path="reliability.html"><a href="reliability.html#reliability-and-the-effect-of-combining-scores-from-separate-tests"><i class="fa fa-check"></i><b>4.2.6</b> Reliability and the Effect of Combining Scores from Separate Tests</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reliability.html"><a href="reliability.html#assumptions-of-internal-consistency-methods"><i class="fa fa-check"></i><b>4.3</b> Assumptions of Internal Consistency Methods</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="reliability.html"><a href="reliability.html#unidimensionality"><i class="fa fa-check"></i><b>4.3.1</b> Unidimensionality</a></li>
<li class="chapter" data-level="4.3.2" data-path="reliability.html"><a href="reliability.html#reliability-models"><i class="fa fa-check"></i><b>4.3.2</b> Reliability Models</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="reliability.html"><a href="reliability.html#GTHEORY"><i class="fa fa-check"></i><b>4.4</b> Generalisability Theory</a></li>
<li class="chapter" data-level="4.5" data-path="reliability.html"><a href="reliability.html#using-reliability-coefficients-to-calculate-confidence-intervals"><i class="fa fa-check"></i><b>4.5</b> Using Reliability coefficients to Calculate Confidence Intervals</a></li>
<li class="chapter" data-level="4.6" data-path="reliability.html"><a href="reliability.html#calculating-the-standard-error-of-measurement-sem"><i class="fa fa-check"></i><b>4.6</b> Calculating the Standard Error of Measurement (SEM)</a></li>
<li class="chapter" data-level="4.7" data-path="reliability.html"><a href="reliability.html#indicative-reading"><i class="fa fa-check"></i><b>4.7</b> Indicative Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="validity.html"><a href="validity.html"><i class="fa fa-check"></i><b>5</b> The validity of measurement</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="validity.html"><a href="validity.html#content-validity"><i class="fa fa-check"></i><b>5.0.1</b> Content validity</a></li>
<li class="chapter" data-level="5.0.2" data-path="validity.html"><a href="validity.html#criterion-related-validity"><i class="fa fa-check"></i><b>5.0.2</b> Criterion-related validity</a></li>
<li class="chapter" data-level="5.0.3" data-path="validity.html"><a href="validity.html#construct-validity"><i class="fa fa-check"></i><b>5.0.3</b> Construct validity</a></li>
<li class="chapter" data-level="5.1" data-path="validity.html"><a href="validity.html#the-relationship-between-reliability-and-validity"><i class="fa fa-check"></i><b>5.1</b> The relationship between reliability and validity:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="outcomes.html"><a href="outcomes.html"><i class="fa fa-check"></i><b>6</b> Tests, Decisions and Outcomes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="outcomes.html"><a href="outcomes.html#test-validity-selection-ratio-base-rate-and-the-quality-of-decisions."><i class="fa fa-check"></i><b>6.1</b> Test Validity, Selection Ratio, Base Rate and the Quality of Decisions.</a></li>
<li class="chapter" data-level="6.2" data-path="outcomes.html"><a href="outcomes.html#calculating-the-probabilities-of-decision-outcomes"><i class="fa fa-check"></i><b>6.2</b> Calculating the Probabilities of Decision Outcomes</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="outcomes.html"><a href="outcomes.html#what-is-the-incremental-validity"><i class="fa fa-check"></i><b>6.2.1</b> What is the Incremental validity?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="outcomes.html"><a href="outcomes.html#calculating-the-cash-benefits-of-a-selection-strategy"><i class="fa fa-check"></i><b>6.3</b> Calculating the Cash Benefits of a Selection Strategy</a></li>
<li class="chapter" data-level="6.4" data-path="outcomes.html"><a href="outcomes.html#indicative-reading-1"><i class="fa fa-check"></i><b>6.4</b> Indicative Reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="TBF.html"><a href="TBF.html"><i class="fa fa-check"></i><b>7</b> Test Bias and Fairness</a>
<ul>
<li class="chapter" data-level="7.1" data-path="TBF.html"><a href="TBF.html#the-potential-for-adverse-impact"><i class="fa fa-check"></i><b>7.1</b> The Potential for Adverse Impact</a></li>
<li class="chapter" data-level="7.2" data-path="TBF.html"><a href="TBF.html#how-to-investigate-test-bias"><i class="fa fa-check"></i><b>7.2</b> How to investigate test bias</a></li>
<li class="chapter" data-level="7.3" data-path="TBF.html"><a href="TBF.html#content-validity-and-dif-analysis"><i class="fa fa-check"></i><b>7.3</b> Content Validity and DIF Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="TBF.html"><a href="TBF.html#the-fair-use-of-tests"><i class="fa fa-check"></i><b>7.4</b> The Fair Use of Tests</a></li>
<li class="chapter" data-level="7.5" data-path="TBF.html"><a href="TBF.html#sources-of-information-and-advice"><i class="fa fa-check"></i><b>7.5</b> Sources of Information and Advice</a></li>
<li class="chapter" data-level="7.6" data-path="TBF.html"><a href="TBF.html#from-the-website"><i class="fa fa-check"></i><b>7.6</b> From the website:</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="TBF.html"><a href="TBF.html#whats-included-in-the-equality-act"><i class="fa fa-check"></i><b>7.6.1</b> What’s included in the Equality Act?</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="TBF.html"><a href="TBF.html#indicative-academic-reading"><i class="fa fa-check"></i><b>7.7</b> Indicative Academic Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="TBG.html"><a href="TBG.html"><i class="fa fa-check"></i><b>8</b> Test Feedback Guidance</a></li>
<li class="chapter" data-level="9" data-path="SP.html"><a href="SP.html"><i class="fa fa-check"></i><b>9</b> Statistics Primer</a>
<ul>
<li class="chapter" data-level="9.1" data-path="SP.html"><a href="SP.html#frequency-distributions"><i class="fa fa-check"></i><b>9.1</b> Frequency distributions</a></li>
<li class="chapter" data-level="9.2" data-path="SP.html"><a href="SP.html#drawing-frequency-distributions"><i class="fa fa-check"></i><b>9.2</b> Drawing frequency distributions</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="SP.html"><a href="SP.html#grouped-frequency-distribution"><i class="fa fa-check"></i><b>9.2.1</b> Grouped frequency distribution</a></li>
<li class="chapter" data-level="9.2.2" data-path="SP.html"><a href="SP.html#skewed-distributions"><i class="fa fa-check"></i><b>9.2.2</b> Skewed distributions</a></li>
<li class="chapter" data-level="9.2.3" data-path="SP.html"><a href="SP.html#the-normal-distribution-curve"><i class="fa fa-check"></i><b>9.2.3</b> The normal distribution curve</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="SP.html"><a href="SP.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>9.3</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="SP.html"><a href="SP.html#mode-median-mean"><i class="fa fa-check"></i><b>9.3.1</b> Mode, Median, Mean</a></li>
<li class="chapter" data-level="9.3.2" data-path="SP.html"><a href="SP.html#when-to-use-the-mode-median-or-mean"><i class="fa fa-check"></i><b>9.3.2</b> When to use the mode, median or mean</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="SP.html"><a href="SP.html#measures-of-dispersion-or-variability"><i class="fa fa-check"></i><b>9.4</b> Measures of dispersion or variability</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="SP.html"><a href="SP.html#the-concept-of-variability"><i class="fa fa-check"></i><b>9.4.1</b> The concept of variability</a></li>
<li class="chapter" data-level="9.4.2" data-path="SP.html"><a href="SP.html#range-interquartile-range"><i class="fa fa-check"></i><b>9.4.2</b> Range; Interquartile range</a></li>
<li class="chapter" data-level="9.4.3" data-path="SP.html"><a href="SP.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>9.4.3</b> Variance and standard deviation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html"><i class="fa fa-check"></i><b>10</b> z scores and percentiles</a>
<ul>
<li class="chapter" data-level="10.0.1" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#standard-deviation-of-the-normal-curve-z-scores"><i class="fa fa-check"></i><b>10.0.1</b> Standard deviation of the normal curve: z scores</a></li>
<li class="chapter" data-level="10.0.2" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#using-normal-curve-z-tables"><i class="fa fa-check"></i><b>10.0.2</b> Using normal curve (z) tables</a></li>
<li class="chapter" data-level="10.0.3" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#comparing-scores-using-z-values"><i class="fa fa-check"></i><b>10.0.3</b> Comparing scores using z values</a></li>
<li class="chapter" data-level="10.0.4" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#z-scores-and-percentiles-1"><i class="fa fa-check"></i><b>10.0.4</b> z-scores and percentiles</a></li>
<li class="chapter" data-level="10.0.5" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#calculating-percentiles-from-a-frequency-distribution"><i class="fa fa-check"></i><b>10.0.5</b> Calculating percentiles from a frequency distribution</a></li>
<li class="chapter" data-level="10.0.6" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#t-scores-stens-and-stanines"><i class="fa fa-check"></i><b>10.0.6</b> T scores, stens and stanines</a></li>
<li class="chapter" data-level="10.1" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#standard-error-and-confidence-limits"><i class="fa fa-check"></i><b>10.1</b> Standard error and confidence limits</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#the-concept-of-standard-error"><i class="fa fa-check"></i><b>10.1.1</b> The concept of standard error</a></li>
<li class="chapter" data-level="10.1.2" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#standard-error-of-measurement"><i class="fa fa-check"></i><b>10.1.2</b> Standard error of measurement</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#correlation-coefficients-and-scattergrams"><i class="fa fa-check"></i><b>10.2</b> Correlation coefficients and scattergrams</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#the-concept-of-correlation"><i class="fa fa-check"></i><b>10.2.1</b> The concept of correlation</a></li>
<li class="chapter" data-level="10.2.2" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#scattergrams"><i class="fa fa-check"></i><b>10.2.2</b> Scattergrams</a></li>
<li class="chapter" data-level="10.2.3" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#pearson-product-moment-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Pearson product moment correlation</a></li>
<li class="chapter" data-level="10.2.4" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#spearman-rank-correlation-rho"><i class="fa fa-check"></i><b>10.2.4</b> Spearman Rank correlation (rho)</a></li>
<li class="chapter" data-level="10.2.5" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#effect-of-range-on-correlation"><i class="fa fa-check"></i><b>10.2.5</b> Effect of range on correlation</a></li>
<li class="chapter" data-level="10.2.6" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#regression"><i class="fa fa-check"></i><b>10.2.6</b> Regression</a></li>
<li class="chapter" data-level="10.2.7" data-path="z-scores-and-percentiles.html"><a href="z-scores-and-percentiles.html#multiple-regression"><i class="fa fa-check"></i><b>10.2.7</b> Multiple regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="RTS.html"><a href="RTS.html"><i class="fa fa-check"></i><b>11</b> Road to (Assignment) Submission</a>
<ul>
<li class="chapter" data-level="11.1" data-path="RTS.html"><a href="RTS.html#general"><i class="fa fa-check"></i><b>11.1</b> General</a></li>
<li class="chapter" data-level="11.2" data-path="RTS.html"><a href="RTS.html#short-glossary"><i class="fa fa-check"></i><b>11.2</b> Short Glossary</a></li>
<li class="chapter" data-level="11.3" data-path="RTS.html"><a href="RTS.html#TM"><i class="fa fa-check"></i><b>11.3</b> Part 1 - The Technical Manual</a>
<ul>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#starting-points"><i class="fa fa-check"></i>Starting Points</a></li>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#introduction-section"><i class="fa fa-check"></i>Introduction Section</a></li>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#method-section"><i class="fa fa-check"></i>Method Section</a></li>
<li class="chapter" data-level="11.3.1" data-path="RTS.html"><a href="RTS.html#checklist-for-part-1"><i class="fa fa-check"></i><b>11.3.1</b> Checklist for Part 1</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="RTS.html"><a href="RTS.html#part-2---sample-feedback"><i class="fa fa-check"></i><b>11.4</b> Part 2 - Sample Feedback</a>
<ul>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#starting-points-1"><i class="fa fa-check"></i>Starting Points</a></li>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#the-report"><i class="fa fa-check"></i>The Report</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="RTS.html"><a href="RTS.html#part-3---critical-reflection"><i class="fa fa-check"></i><b>11.5</b> Part 3 - Critical Reflection</a></li>
<li class="chapter" data-level="11.6" data-path="RTS.html"><a href="RTS.html#appendix"><i class="fa fa-check"></i><b>11.6</b> Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="RTS.html"><a href="RTS.html#checklist-for-appendix"><i class="fa fa-check"></i>Checklist for Appendix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="SCN.html"><a href="SCN.html"><i class="fa fa-check"></i><b>12</b> Scenarios</a>
<ul>
<li class="chapter" data-level="12.1" data-path="SCN.html"><a href="SCN.html#psychometric-consultancy-at-birley-house"><i class="fa fa-check"></i><b>12.1</b> Psychometric consultancy at Birley House</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="SCN.html"><a href="SCN.html#the-situation"><i class="fa fa-check"></i><b>12.1.1</b> The Situation</a></li>
<li class="chapter" data-level="12.1.2" data-path="SCN.html"><a href="SCN.html#managers-letter"><i class="fa fa-check"></i><b>12.1.2</b> Manager’s Letter</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="SCN.html"><a href="SCN.html#psychometric-consultancy-at-hmp-birley"><i class="fa fa-check"></i><b>12.2</b> Psychometric consultancy at HMP Birley</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="SCN.html"><a href="SCN.html#the-situation-1"><i class="fa fa-check"></i><b>12.2.1</b> The Situation</a></li>
<li class="chapter" data-level="12.2.2" data-path="SCN.html"><a href="SCN.html#letter-from-the-governor"><i class="fa fa-check"></i><b>12.2.2</b> Letter from the Governor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="misc.html"><a href="misc.html"><i class="fa fa-check"></i><b>13</b> Miscellaneous</a>
<ul>
<li class="chapter" data-level="13.1" data-path="misc.html"><a href="misc.html#mischead"><i class="fa fa-check"></i><b>13.1</b> What Do You Mean by Sub-Heading Levels?</a>
<ul>
<li class="chapter" data-level="" data-path="misc.html"><a href="misc.html#level-1-major-section-introduction-etc"><i class="fa fa-check"></i>Level 1 (Major Section, Introduction etc)</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="misc.html"><a href="misc.html#BCID"><i class="fa fa-check"></i><b>13.2</b> Boundary Conditions In-depth</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Twisted Pear</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reliability" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> The Reliability of Measurement<a href="reliability.html#reliability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Tests typically contain many items for measuring a construct. As has previously been mentioned, these items are a sampling of relevant questions/items from the much greater number of items that potentially could be used to measure the construct. A number of important questions arise from this: how many items should be included in the test, how diverse should they be, and how can we tell if they have a common core? These questions, as we shall see, are about building a test that gives consistent or reliable measurement. Reliability enables meaningful measurement. Furthermore, even if we can answer these questions, how can we know if the test is any good for its intended use, that is, whether the test is measuring something meaningful and useful? This is a matter of establishing the validity of measurement.</p>
<div id="reliability-1" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Reliability<a href="reliability.html#reliability-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Firstly, to address how many items should be included on a test and how diverse should they be. Take, for example, a test intended to measure mathematical reasoning. If the test is to measure reasoning then its items should involve reasoning or ‘figuring things out’, and for a test of mathematical reasoning the item content should have a mathematical content or relevance. So far, this is obvious, but there are a many sorts of problems that involve reasoning. These may include:</p>
<p>Figuring out analogies such as:</p>
<p>60 is to 20 as 120 is to?</p>
<p>Figuring out the odd term such as:</p>
<p>362, 144, 313, 242</p>
<p>Figuring out the sequence such as:</p>
<p>7, 10, 16, 28…</p>
<p>Thus, there are very many ways in which to present a reasoning problem, and within each way there are many different mathematical operations that could be included (e.g. just addition, just subtraction, just division, just multiplication or any combination of these operations). Why not just take one way of presenting the problem and one kind a mathematical operation, perhaps using just a few items on the test? This would be a very poor measure because:</p>
<div id="few-items-will-not-represent-the-whole-domain-of-the-construct" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Few items will not represent the whole domain of the construct<a href="reliability.html#few-items-will-not-represent-the-whole-domain-of-the-construct" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For example, consider a test of mathematical reasoning that just included items of sequences involving multiplication. Unless, for everybody, this type of content is very highly correlated with other ways of mathematical reasoning, we will not have measured all that we mean by mathematical reasoning. Thus we should include items from the whole range of possible item content so as to represent the breadth of the construct we intend to measure; the broader the construct the more items we will need.</p>
</div>
<div id="difficulty-levels-cannot-be-properly-varied-with-few-items-and-so-cannot-reveal-a-range-a-difference-between-people." class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Difficulty levels cannot be properly varied with few items and so cannot reveal a range a difference between people.<a href="reliability.html#difficulty-levels-cannot-be-properly-varied-with-few-items-and-so-cannot-reveal-a-range-a-difference-between-people." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Thus, the more items there are on a test and the more varied these are in difficulty, then the more potentially discriminating the test will be.</p>
<p>Furthermore, any single item is likely to be a poor measure because of the various sources of random error that can affect performance on one item. Such sources include:</p>
</div>
<div id="any-one-item-may-be-poorly-constructed" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Any one item may be poorly constructed<a href="reliability.html#any-one-item-may-be-poorly-constructed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Any specific item may be a poor measure because it is badly constructed. For example, it may be just too easy or difficult or it may include irrelevant content. Thus, the more items there are on a test, the less influence there will be from one or two poor items.</p>
</div>
<div id="any-one-item-may-offer-advantage-or-disadvantage-to-the-test-taker-because-of-their-previous-experience" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Any one item may offer advantage or disadvantage to the test taker because of their previous experience<a href="reliability.html#any-one-item-may-offer-advantage-or-disadvantage-to-the-test-taker-because-of-their-previous-experience" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For any specific item, an individual may advantaged or disadvantaged by their previous level of familiarity with just this type of reasoning, and/or just this type of content. The item may, for example, just hit a ‘blind spot’. Thus, the more diverse the items are, the more likely it is that such influences will balance out.</p>
</div>
<div id="any-one-item-may-be-affected-by-very-temporary-events" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> Any one item may be affected by very temporary events<a href="reliability.html#any-one-item-may-be-affected-by-very-temporary-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When attempting to answer any single item the person may be affected by external distractions (e.g. noise, the person next to them, etc) or internal distractions (e.g. suddenly wondering if they left the iron on). Lucky guesses could also be placed in this category. Thus the longer the test the less influence very temporary events will have on the total test score.</p>
<p>To give reliable or consistent measurement, a test thus generally needs many diverse items to measure a construct so as to balance out the various sources of random error outlined above.</p>
</div>
</div>
<div id="reliability-and-classical-test-theory" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Reliability and Classical Test Theory<a href="reliability.html#reliability-and-classical-test-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The above discussion of the need for many diverse items is reflected in Classical Test Theory. This states that the response to any item within a test can be thought of as being made up of two contributing influences: the response is partly due to the person’s true ability or personality (or whatever construct is being measured) and partly due to the various sources of random error outlined previously. That is:</p>
<p><span class="math inline">\(Item Score (X) = True Score + Random Error\)</span></p>
<p>If this is the case, then by adding together lots of item scores the random error will increasingly cancel itself out and the total score across the items will increasingly reflect a person’s true score. That is:</p>
<p>Total Score across Items (X) = True Score (and more true score than at an item level) + Random Error (and less random error than at an item level)</p>
<p>The True score (T) is the score the person would obtain if they were tested using all of the possible items from the relevant construct domain. As such it is a hypothetical score.</p>
<p>Clearly, to be representative, broad constructs will need more items than narrow constructs. Furthermore, because this usually involves sampling many items, any random errors within individual items are likely to balance out over the whole collection of items within the test. However, this diversity of item content will only increase accuracy of construct measurement if the items falls within the construct domain (e.g. mathematical reasoning), that is, if there is a common core to the items.</p>
<div id="determining-the-reliability-coefficients" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Determining the Reliability Coefficients<a href="reliability.html#determining-the-reliability-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In general then, the more items a test contains, the more reliable will be the measurement i.e. the less error there will be. However, good items are not easy to write, and of course the practical use of the test means that they should not take too long to complete. Kline (1993) claims that a 10 item test can give quite consistent measurement, and a 20 item test can give very satisfactory reliability. These are very much rough guides, to know with any confidence we should determine the reliability of the test, and for this we need to calculate a reliability coefficient.
Classical reliability theory shows that a test’s reliability is equal to the average inter-correlation of all possible same length tests that sample the same item content. This average inter-correlation gives the reliability coefficient (<span class="math inline">\(r_{xx}\)</span>) for the test.</p>
<p>Perhaps the ideal way of calculating a reliability coefficient would be:</p>
<p>• To construct many parallel versions of a test (i.e. many same length tests that each sample items from the same construct domain)
• Have a very large group of people (i.e. a norm group) take each parallel version, for example everyone takes one version of the test every week for a month
• Correlate the norm group test scores between every test and calculate an average correlation coefficient between the tests
Such an average correlation coefficient would give a very useful index of the extent to which test scores change due to item sampling (item sampling error) and due to time of testing (time sampling error)</p>
<p>The reliability coefficient can be thought of as representing the proportion of true score variance (<span class="math inline">\(\sigma^2t\)</span>) to observed score variance (<span class="math inline">\(\sigma^{2}x\)</span>).</p>
<p>Thus, if we have a reliability coefficient of 0.8 then 0.8 or 80% of the tests variance is attributable to true score variance and 20% is due to random error.</p>
<p>Thus the greater the reliability coefficient then the more the scores on a test will reflect ‘true’ scores, and the more consistent will be the measurement. Calculating the reliability coefficient would involve inter-correlating the scores from all possible same length tests from the relevant construct domain. In practice, reliability coefficients are estimated from the inter-correlation of some of the tests from the relevant construct domain. These practical methods are listed below:</p>
<ul>
<li>Test-Retest methods</li>
<li>Alternate Form methods</li>
<li>Split-half methods</li>
<li>Internal Consistency methods</li>
<li>Scorer Reliability methods</li>
</ul>
<p>Each of these methods gives information on the reliability of measurement, although what is meant by the term consistency is rather different in each case.</p>
</div>
<div id="test-retest-methods" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Test-Retest Methods<a href="reliability.html#test-retest-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>On two occasions, give the test to a large sample (n &gt; 100) of people for whom the test is intended; score the test for the two occasions, then correlate the two sets of scores. If the sample has very similar test scores on the two occasions, the correlation coefficient will be high, and there is evidence for consistency of measurement over time. This method has the advantage of being conceptually simple, but there are some difficulties, for example:</p>
<div id="how-long-a-gap-between-test-and-re-test" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> How long a gap between test and re-test?<a href="reliability.html#how-long-a-gap-between-test-and-re-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If it is too short the respondents may just remember their previous responses. Kline (1993) recommends a gap of at least three months between retests. If, however, it is too long, respondents may actually have changed with regard to the construct being measured. This is especially likely to be the case for children rather than older people. Anastasi (1997) suggests that, for any group of respondents, the interval between retests should rarely exceed six months. In all cases the test manual should state the interval used.</p>
</div>
<div id="which-constructs-are-suitable-for-test-retest-methods" class="section level4 hasAnchor" number="4.2.2.2">
<h4><span class="header-section-number">4.2.2.2</span> Which constructs are suitable for test-retest methods?<a href="reliability.html#which-constructs-are-suitable-for-test-retest-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Measures of attitude and current state (e.g. anxiety) may be less suitable than measures of personality and ability.</p>
<p>Kline (1993) suggests that a correlation of 0.8 is a minimum figure for test-retest reliabilities.</p>
</div>
</div>
<div id="alternate-form-methods-parallel-form-methods" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Alternate Form Methods (Parallel form methods)<a href="reliability.html#alternate-form-methods-parallel-form-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, it is necessary to construct different versions of the same test (i.e. tests the have same content domain, same mean and same variance). Then, on two occasions, give different versions of the same test to a large sample (n &gt; 100) of people for whom the test is intended, and then correlate the two sets of scores.</p>
<p>This method avoids the difficulty of people simply remembering their previous responses, and there can therefore be small time gaps between administrations. It is, however, difficult and expensive to properly construct alternate forms, and there are few available. Also, there still may be a problem with general practice or sensitising effects. This would not be a problem if everybody “benefits” to the same extent (such systematic bias will not affect the correlation), however it is more usual for some people to benefit more than others.</p>
</div>
<div id="split-half-methods" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Split-half Methods<a href="reliability.html#split-half-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, the correlation is between two halves of one test. This has the advantage of only needing one testing session and only one test. However, there is the problem of how to split the test in two for purposes of correlation. Furthermore, the obtained correlation is for a test that is half as long as the actual test, and as such will be an underestimation of the reliability of the test. The Spearman-Brown formula may be used to correct for this underestimation:</p>
<p><span class="math inline">\(r_{est} = \frac{2r_{ob}}{1+r_{ob}}\)</span></p>
<p>Where:</p>
<p><span class="math inline">\(r_{est} =\)</span> estimated correlation of full test</p>
<p><span class="math inline">\(r_{ob}\)</span> = obtained correlation from the split half</p>
<p>The above formula is actually an application of the more general formula for estimating the effect of lengthening a test</p>
<p><span class="math inline">\(\frac{r’=n r_{xx}}{1+(n-1 r_{xx})}\)</span></p>
<p>Where:</p>
<p><span class="math inline">\(r’\)</span> = estimated reliability of changed test
n = factor by which the test length is increased
<span class="math inline">\(r_{xx}\)</span> = reliability of the original test</p>
</div>
<div id="internal-consistency-methods" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Internal Consistency Methods<a href="reliability.html#internal-consistency-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>These methods estimate the reliability of the test from the number of items in the test and the average inter-correlation among the items. Such internal consistency estimates can be shown to be equivalent to the mean of all possible corrected split-halves for a test of a given length. This means that internal consistency methods give the average correlation between all possible same length versions of the test content. As such, internal consistency methods are considered to be the best estimate of the theoretical correlation between all possible pairs of tests in the domain, however such methods do not take account of error due to time of testing (i.e. time sampling error).</p>
<p>The formulas for calculating internal consistency take into account the number of items on the test (i.e. length of the test); because the more items there are on a test then the more likely it is to balance out the influence of random error. However, this will only be the case if the items actually share a common core and therefore the formulas also take into account the extent to which there is some consistency of responses between the items by using the average inter-item correlation.</p>
<p>Coefficient alpha is used for calculating internal consistency when items have more than two response options (e.g. personality scales)</p>
<p><span class="math display">\[
r_{xx} = \frac{n \times r_{averagejj}}{1+ (n-1) r_{averagejj}}
\]</span></p>
<p>Where:
<span class="math inline">\(r_{xx}\)</span> = reliability of the test (internal consistency)</p>
<p>n = number of items on test</p>
<p><span class="math inline">\(r_{average}jj\)</span> = average inter-correlation of the test items</p>
<p>The more conventional way of calculating coefficient alpha (which also takes account of any differences in standard deviations across items) is shown below:</p>
<p><span class="math inline">\(r_{xx} = \frac{n}{n-1} (1-\frac{\Sigma sd^2_i}{sd^2x_i})\)</span></p>
<p>Where:
<span class="math inline">\(r_{xx}\)</span> = coefficient alpha
n = number of items on the test
<span class="math inline">\(∑sd2i\)</span> = the sum of the item variances
<span class="math inline">\(sd2x\)</span> = the variance of scores on the total test</p>
<p>Kuder-Richardson formula 20 (KR-20) is used when items are scored 0 or 1 (e.g. right or wrong):</p>
<p><span class="math inline">\(r_{xx} = \frac{n}{n-1}\times\frac{1-\Sigma p_q}{sd^2_x)}\)</span></p>
<p>Where:</p>
<p><span class="math inline">\(r_{xx}\)</span> = KR-20</p>
<p>n = number of items on the test</p>
<p>p = proportion of test takers getting each item right</p>
<p>q = proportion of test takers getting each item wrong</p>
<p><span class="math inline">\(sd^2_x\)</span> = the variance of scores on the total test</p>
<p>An important implication of these formulas, is that for two tests of the same length, the more reliable test must have greater variation in the scores across test takers and so be more discriminating between respondents.</p>
<p>As a general rule, internal consistency coefficients should not drop below 0.7, and ideally be around 0.9 or even higher if used to make important decisions about individuals.</p>
<p><img src="Floats/Ch3/Tab1.png" width="252" /></p>
</div>
<div id="reliability-and-the-effect-of-combining-scores-from-separate-tests" class="section level3 hasAnchor" number="4.2.6">
<h3><span class="header-section-number">4.2.6</span> Reliability and the Effect of Combining Scores from Separate Tests<a href="reliability.html#reliability-and-the-effect-of-combining-scores-from-separate-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The higher the correlation between two tests or scales, then the higher will be the reliability of two scores combined from these scales. Thus composite scores are typically more reliable than the individual tests, and their reliability can be calculated from the correlation between the tests and the average test reliability.</p>
<p>Furthermore, the higher the correlation between two scales then the lower will be the reliability of the difference score. Difference scores reflect differences in true scores and differences due to measurement error. With highly correlated scales there will be little difference in the true scores (since both are measuring the same construct), thus any difference between two scale scores is likely to be due to measurement error. The reliability of difference scores can be calculated from the reliability of the two tests and their correlation.</p>
</div>
</div>
<div id="assumptions-of-internal-consistency-methods" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Assumptions of Internal Consistency Methods<a href="reliability.html#assumptions-of-internal-consistency-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the same way that we need to check assumptions when running statistical analyses on our data, we also need to check assumptions when establishing the internal consistency of our measures. Whilst Cronbach’s Alpha is not the most conservative of measures of internal consistency, it is still more restrictive than we might like. There are three reliability models, each with progressively fewer restrictions.</p>
<div id="unidimensionality" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Unidimensionality<a href="reliability.html#unidimensionality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First and foremost, any time we use a measure, we need to ensure that it is <em>Uni-dimensional</em>, that is only measuring a single factor, rather than a series of subscales. This is obvious when thinking about <em>tests</em> - we would not purposefully include verbal intelligence items in a spatial reasoning task - but less so when considering inventories such as Portrait-Values or Personality measures where questions might be ambiguous, or relate to many different constructs. In this regard it is always important to check for dimensionality when using inventory measures. Common ways to do this are exploratory factor analysis and parallel analysis.</p>
</div>
<div id="reliability-models" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Reliability Models<a href="reliability.html#reliability-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Unidimensionality is common to all reliability models. If a measure is formed of subscales, then these ought to be checked separately. After Unidimensionality has been established, the model that is selected will depend on the structure of the factor and the features of the items.</p>
<div id="parallel" class="section level4 hasAnchor" number="4.3.2.1">
<h4><span class="header-section-number">4.3.2.1</span> Parallel<a href="reliability.html#parallel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Parallel reliability models are the most restrictive. They assume that not only do items have the same covariances, i.e. they relate to each other in the same way, but also that they have the same variance. We saw earlier that parallel forms reliability requires sets of items that are ostensibly identical in all but wording, and this is what that would look like. This is perhaps easier to achieve when making tests: 1 + 2, and 2 + 1 are parallel in that they have the same elements, the same answer, but are just rearranged. When making inventories however, it is much more difficult: can we really say that two items are identical if they mention different objects? Take for instance ‘I generally vote for liberal candidates’ and ‘I generally vote for conservative candidates’ - to say these are parallel would imply that the correlation between them was about -1, i.e. a person who scores 1 on the former will always score 5 on the latter - but that is unlikely since there will be variation in how people interpret the words conservative or liberal. These may have asymmetric meanings - a conservative might vote for a left leaning candidate under some circumstances, whereas a liberal voter might never vote for a conservative regardless of the alternative.</p>
</div>
<div id="tau-esesentially-tau-equivalent" class="section level4 hasAnchor" number="4.3.2.2">
<h4><span class="header-section-number">4.3.2.2</span> Tau/ Esesentially Tau Equivalent<a href="reliability.html#tau-esesentially-tau-equivalent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Tau equivalnce allows the variances of items to change, but assumes that the covariances remain the same - that is, were the population to be sampled, the covariances between items would be identical (so sample covariances should be similar, give or take sampling error). This is perhaps a little too much to ask, and so Essential Tau equivalence, where each item is assumed to have a common true score, is used instead.</p>
</div>
<div id="congeneric" class="section level4 hasAnchor" number="4.3.2.3">
<h4><span class="header-section-number">4.3.2.3</span> Congeneric<a href="reliability.html#congeneric" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Congeneric reliability is the least restrictive model of reliability. It literaly means ‘of the same genus’, so rather than only measuring lions, one could also measure attitudes towards tigers, cheetahs, and Bobcats too. Congeneric reliability allows both variances and covariances of items to vary. They still need to be correlated with the same factor, but the strength of correlation is free to vary.</p>
</div>
</div>
</div>
<div id="GTHEORY" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Generalisability Theory<a href="reliability.html#GTHEORY" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Classical reliability theory considers inconsistencies in test scores to be random error, and the different sources of random error are identified by the various methods for establishing reliability. In contrast, generalisability theory attempts to identify specific and systematic sources of inconsistency (e.g. using a test for selection compared to using the same test for career development). Here the concern is with the extent to which test scores from one situation are generalisable to scores in other situations. Within generalisability theory, whether a test is reliable depends upon the conditions of measurement and the intended use of the test scores<br />
For example, if we wish to generalise a test score to how people perform under pressure then testing people with harsh supervisors is not a source of random error.</p>
</div>
<div id="using-reliability-coefficients-to-calculate-confidence-intervals" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Using Reliability coefficients to Calculate Confidence Intervals<a href="reliability.html#using-reliability-coefficients-to-calculate-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As discussed previously, classical test theory states that any measurement can be thought of as being made up of two parts: the person’s true score for that attribute and some error component.
The True score (T) is the score the person would obtain if they were tested using all of the possible items from the relevant content domain (i.e. no content sampling error). As such it is a hypothetical score.</p>
<p>In this theory all important errors in measurement are considered to be random and normally distributed. Thus if a person were to be given a subset of test items repeatedly, and if we were able to achieve no carry-over effects from one test session to the next, then we would expect a distribution of obtained scores the mean of which would approximate the person’s true score. This is obviously a hypothetical situation.</p>
<p><img src="Floats/Ch3/Fig1.png" width="172" /></p>
<p>Clearly, the more random error associated with a particular test then the greater will be the variance (and standard deviation) of this hypothetical distribution. Although it is impossible to actually repeatedly test an individual in such a way, the standard deviation of this hypothetical distribution can be estimated from knowledge of the reliability coefficient for the test and the standard deviation of obtained scores for a population of test takers (i.e. the norm group). This estimate of the standard deviation for the above distribution is termed the Standard Error of Measurement (SEM).</p>
</div>
<div id="calculating-the-standard-error-of-measurement-sem" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Calculating the Standard Error of Measurement (SEM)<a href="reliability.html#calculating-the-standard-error-of-measurement-sem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The SEM may be calculated as follows:</p>
<p><span class="math display">\[\large SEM = SD\sqrt{1-r_{xx})}\]</span></p>
<p>Where: SD is the standard deviation for the test scores obtained by the norm group (or the SD of transformed scores e.g. T scores) and <span class="math inline">\(r_{xx}\)</span> is the reliability coefficient for this same group.</p>
<p>The SEM is a useful statistic because it allows us to put concrete confidence bands around any observed score. For example, if the obtained ability raw score of a person is, for example, 100, and the SEM for these raw scores was 5, then we could be 95% confident that his or her true score on this test is likely to fall within 9.8 (i.e. 1.96 x 5) points of this obtained score i.e. the person’s true score is likely to be between 90.2 and 109.8 (i.e. 100 - 9.8 to 100 + 9.8).</p>
<p>The logic here is as follows: if, as discussed above, 95% of observed scores will fall within about 2 SEM (1.96 SEM to be precise) of the true score, then 2 SEM around any given observed score will contain the true score on 95% of occasions. The figure below illustrates a case where the test SEM is 5 points and there is an observed score of 100. The score of 100 is obviously within the 95% confidence band for a true score of 100, but it is also within the 95% confidence band for a true score of 90 and for a true score of 110. Thus an observed score of 100 may come from a true score anywhere between 90 and 110, and this will be the case on 95% of occasions. When using the Standard error of measurement we are constructing confidence bands on the basis of the likely test score range for any given true score.</p>
<p><img src="Floats/Ch3/Fig2.png" width="206" /></p>
<p>In the last example, a 95% confidence band was calculated. By using normal distribution curve (NDC) tables it is possible to calculate any level of confidence band for a given score.</p>
<p>Norm tables within test manuals often give 68% confidence bands for any obtained score (i.e. 1 SEM around the observed score). .</p>
<div id="calculating-the-standard-error-of-difference-sediff" class="section level4 hasAnchor" number="4.6.0.1">
<h4><span class="header-section-number">4.6.0.1</span> Calculating the Standard Error of Difference (SEdiff)<a href="reliability.html#calculating-the-standard-error-of-difference-sediff" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As well as placing confidence bands around individual scores, reliability coefficients may also be used to assess confidence about differences in test score. For example, if a person scores differently on two tests, how confident can we be that this is a real difference in performance rather than just measurement error associated with both tests? A similar question is: if two people score differently on the same test, how confident can we be that they actually do differ in underlying ability rather than this being the result of error within the test? To answer these questions we can use the Standard error of Difference (SEdiff) as follows:</p>
<p><span class="math display">\[
\large SEdiff = \sqrt{(SEM^2_{test1} + SEM^2_{test2})}
\]</span></p>
<p>The standard error of difference can be seen as the standard deviation of the distribution of differences scores, and involves the SEM of both tests. These two tests could either be the same test on two different people (in which case the SEM above will be the same on both tests) or it could be two different tests on the same person (in which case the SEM above will be different for test 1 compared to test 2). In either case, the application of the above formula requires that test 1 and 2 are using a comparable measurement scale i.e. the tests have the same mean and standard deviation. This means that the above formula may be rewritten as shown below:</p>
<p><span class="math display">\[ \large SEdiff =  sd\sqrt{(2 – r_{xxtest1}  -  r_{xxtest2}}\]</span></p>
<p>If we were comparing performance on two different tests we must use one of the standardised scoring systems. If we used the T scoring system, this would give both tests a standard deviation of 10. Then if, for example, test 1 has a reliability of 0.75 and test 2 has a reliability of 0.85, then the SEdiff may be calculated as follows:</p>
<p><span class="math inline">\(SEdiff =  10\sqrt{(2 - 0.75  - 0.85)}\)</span>
<span class="math inline">\(=  10 \times 0.63\)</span>
<span class="math inline">\(=  6.3\)</span></p>
<p>Thus, since the SEdiff is a unit of standard deviation, we can be 68% confident that if the two test scores differ by as much as 6 T score points (some rounding is need here) then there is a reliable difference in test score performance. If the two scores differ by as much as 12 T score points (i.e. 1.96 x SEdiff) then we can be 95% confident that there is a reliable difference in performance.</p>
<p>When comparing people on the same test we could use the raw score standard deviation in the calculation of the SEdiff or we could use one of the standardised scoring systems as above (e.g. T scores), In fact, when the same test is involved the SEdiff formula reduces to that shown below:</p>
<p><span class="math inline">\(SEdiff =  \sqrt{(2)} \times SEM\)</span></p>
<p><span class="math inline">\(SEdiff = 1.414 \times SEM\)</span></p>
<p>If, for example, we were using T scores as the basis for comparison and the test had a reliability of 0.8 (a fairly typical level) then the SEdiff would be:</p>
<p><span class="math inline">\(SEdiff = 1.414 \times (10\sqrt{1-0.8})\)</span>
<span class="math inline">\(= 1.414 \times 4.47\)</span>
<span class="math inline">\(= 6.3\)</span></p>
<p>The interpretation of the above statistic is the same as the previous example.</p>
</div>
</div>
<div id="indicative-reading" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Indicative Reading<a href="reliability.html#indicative-reading" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Charter, R.A. (1997). Confidence Interval Procedures for Retest, Alternate-Forms, Validity and Alpha Coefficients. Perceptual and Motor Skills, 84, 1488-1490.</p>
<p>Charter, R.A. &amp; Feldt, L.S. (2000). The Relationship Between Two Methods of Evaluating an Examinee’s Difference Scores. Journal of Psychoeducational Assessment, 18, 125-142.</p>
<p>Charter, R.A. &amp; Feldt, L.S. (2001). Confidence Intervals for True Scores: Is there a Correct Approach. Journal of Psychoeducational Assessment, 19, 350-364.</p>
<p>Charter, R.A. &amp; Feldt, L.S. (2002). The Importance of Reliability as it Relates to True Score Confidence Intervals. Measurement and Evaluation in Counselling and Development, 35, 104-112.</p>
<p>Charter, R.A. &amp; Feldt, L.S. (2009). A Comprehensive Approach to the Interpretation of Difference Scores. Applied Neuropsychology, 16, 23-30.</p>
<p>Charter, R.A. (2009). Differences Scores: Regression-Based Reliable Difference and the Regression-Based Confidence Interval. Journal of Clinical Psychology, 65(4), 456-460.</p>
<p>Cho, E. (2016). Making Reliability Reliable: A Systematic Approach to Reliability Coefficients. Organizational Research Methods, 19(4), 651–682. <a href="https://doi.org/10.1177/1094428116656239" class="uri">https://doi.org/10.1177/1094428116656239</a></p>
<p>Dudek, F.J. (1979). The Continuing Misrepresentation of the Standard Error of Measurement. Psychological Bulletin, 86, 335-337.</p>
<p>Glutting, J.J., McDermott, P.A., &amp; Stanley, J.C. (1987). Resolving Differences among Methods of Establishing Confidence Limits for Test Scores. Educational and Psychological Measurement, 47, 607-614.</p>
<p>Helms, J.E., Henze, K.T., Sass, T.L., Mifsud, V.A. (2006). Treating Cronbach’s Alpha Reliability as Data in Counseling Research. The Counseling Psychologist, 34, 630-660.</p>
<p>Harvill, L. M. (1991). Standard Error of Measurement. Educational Measurement: Issues and Practice, 10, 33–41. <a href="doi:10.1111/j.1745-3992.1991.tb00195.x" class="uri">doi:10.1111/j.1745-3992.1991.tb00195.x</a>
Standard Error of Measurement - NCME
Within the above link: you have used equation 6 in the article for the SEM within your data record sheets, and equation 10 for the SEdiff within your data record sheets (though article calls this the SEM for the score differences)</p>
<p>Mortensen, E. L., &amp; Gade, A. (1992). Linear versus normalized T scores as standardized neuropsychological test scores. Scandinavian Journal of Psychology, 33(3), 230–237.</p>
<p>Tavakol, M., &amp; Dennick, R. (2011). Making sense of Cronbach’s alpha. International Journal of Medical Education, 2, 53–55.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="NDSM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="validity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Rowley.pdf", "Rowley.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
