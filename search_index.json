[["index.html", "The Twisted Pear A very basic introduction to the assessment of abilities Chapter 1 Welcome", " The Twisted Pear A very basic introduction to the assessment of abilities Kevin Rowley 2025-03-24 Chapter 1 Welcome This is an E-Book version of the Ability Assessment module materials that was run at Manchester Metropolitan University by Kevin Rowley until 2018. It is heavily geared towards Occupational Psychology, but many of the concepts apply to other areas of measurement, including personality which the new Psychological Measurement module focusses on. It includes sections that will help with all parts of the Psychological Measurement Portfolio, as well as a helpful examples of how to provide feedback, and statistics primer that has been included at the end of the book. "],["intro.html", "Chapter 2 General Features of Psychological Tests 2.1 Types of Psychological Test 2.2 Using Tests for Occupational Purposes 2.3 The Usefulness of Tests in Selection for Jobs 2.4 Validity Generalization and Meta-analysis 2.5 Implications from Validity Generalization 2.6 Determinants of Job Performance 2.7 What is meant by General Mental Ability (GMA) 2.8 Criterion-related Validity Coefficients and the Coefficient of Determination 2.9 Criterion-Related Validity and Multiple-Correlation 2.10 Indicative General References on Selection", " Chapter 2 General Features of Psychological Tests Psychological tests are commonly described as objective and standardised measures of a sample of behaviour (e.g. Anastasi &amp; Urbina, 1997; Janda, 1998). A test is objective if its administration, scoring and interpretation are clearly specified and replicable. A test is standardised if these specifications are adhered to and uniformly applied. A standardised test often uses a large comparison group (i.e. norm group) for interpretation of test scores. A test is a sample of behaviour if its content or administration involves only some of the possible activities and/or some of the possible circumstances with which the test is concerned. For example, a test might be concerned with mathematical ability. The test may involve several mathematical problems to be solved without a calculator and with a time limit. This test will contain only a sample of the possible mathematical problems that could be included. Furthermore, it samples mathematical problem solving behaviour under quite specific circumstances i.e. without a calculator, with a time limit, and possibly in an anxiety provoking situation. Thus, quite apart from the specific content of the test, the testing situation itself should be viewed as a sample of all the possible circumstances where solving maths problems could be demonstrated. Tests, then, are concerned with samples of behaviour. However, the behaviour being sampled is often regarded as an indirect index or sign of some underlying concept such as ability (e.g. verbal, numerical, spatial) or personality (e.g. extraversion, agreeableness, etc). Such concepts have four features in common: • They are concepts used to summarise and describe observed consistencies in behaviour. • These concepts are usually regarded as theoretical constructs that encompass more than the original set of observations. • There is no single set of questions or items that can be used to measure the construct. There must instead be a sampling of relevant questions/items from the universe of items within the construct domain. • The construct must always be related, either directly or indirectly, to observable behaviour or experience. Tests that claim to measure intelligence are clearly sampling behaviour that is considered to be a sign of a broad theoretical concept. The same may be said of more narrow measurements such as mathematical ability, or even tests that attempt to measure the concept of ‘knowledge’ of a subject area. However, it should be clear that the aspiration of tests to measure broad constructs must always be limited by the fact that they involve only a sample of behaviour which may not be representative. There can be some tests that are narrow enough in their concern that they are often regarded as directly sampling the almost complete behaviour e.g. a typing test, spot welding test and other ‘work’ samples where there is a ‘point-to-point’ correspondence between behaviours assessed through the sample and some of the behaviours required within the job. Of course, there is still problem that the circumstances of testing may be unrepresentative. 2.1 Types of Psychological Test Psychological tests can be broadly categorised according to the type of construct they claim to measure: Intelligence tests: One of the oldest types of psychological test is the ‘intelligence’ test which claims to measure overall basic reasoning ability. Modern versions of these tests include the Wechsler Adult Intelligence Scale (WAIS- IV), the Stanford-Binet test (for ages 4-17) and the British Ability Scales (for ages 2 ½ -17 ½). These three tests have to be administered on an individual basis. Other intelligence tests can be administered to groups e.g. Raven’s Progressive Matrices, The Culture Fair Tests, and the Alice Heim 3 (AH3). Specific Ability tests: The above intelligence tests measure very general ability, other types of test are designed to assess more specific ability factors, the most important of which are considered to be verbal ability, spatial ability and mathematical reasoning (Kline, 2000). For example, the Watson-Glaser Critical Thinking Appraisal measures verbal reasoning and is used within graduate recruitment and management selection. Sub-scales within general intelligence tests may also be considered as tests of specific ability e.g. the arithmetic scale of the WAIS. There are also a number of tests that assess very specific factors e.g. the Purdue Pegboard (a measure of finger, hand and arm dexterity). Aptitude tests: Occasionally, the above specific ability measures are referred to as aptitude tests. However, it is more usual for the term ‘aptitude test’ to be used when the test contains a mixture of underlying abilities needed for a particular course, job role or occupation (e.g. General Clerical Test, Computer aptitude test). Achievement (or attainment) tests: The above ability and aptitude tests often claim to measure the potential for learning or acquiring a new skill, in that they aim to assess necessary underlying abilities. In contrast, achievement tests measure the level of knowledge or skill attained by an individual through instruction (e.g. university examinations). Interest tests and Values tests: Typically tests of interest contain a large number of items about whether various activities, situations and types of people are liked or disliked e.g. the Vocational Interest Measure (VIM), The Strong-Campbell Interest Inventory (SCII). Tests of values attempt to measure a person’s basic philosophy and orientation to the world e.g. The Study of Values, the Rokeach Value Survey. Personality tests: these usually contain a large number of items about feelings and behaviour and attempt to measure fundamental differences in temperament e.g. The Californian Psychological Inventory (CPI), the Myers-Briggs Type indicator (MBTI), The 16 Personality Factors Test (16PF), Quintax. Integrity tests: These tests attempt to identify potentially counterproductive or dishonest employees. Some of these tests may be described as overt tests in that they ask directly about dishonest activities and attitudes towards dishonesty e.g. the London House Personnel Selection Inventory; the Reid Report. Other integrity tests may be described as covert tests in that they aim to measure aspects of personality related to counterproductive behaviour, such as unreliability and carelessness e.g. Giotto 2.1.1 Specific Features of each Type of Test 2.1.1.1 Tests of Maximal Performance Tests of intelligence, specific ability, aptitude and achievement (for which items have a correct answer) are often described as tests of maximal performance (how well a person can do). Such tests most commonly consist of multiple-choice items. Here, an item presents a problem to be solved (termed the stem), followed by a choice of possible answers (typically around five and termed the response set) only one of which is correct (termed the keyed response). The incorrect answers are termed distracters. These tests of maximal performance can be classified as either Power tests, speed tests or speeded tests: Power tests have no time limit, or a limit that is long enough so that about 90% of test takers can attempt all items. Such tests usually have items that gradually increase in difficulty so that, for most people, a limit in the ability to solve the problems is reached. Speed tests have a time limit that prevents all items from being attempted, but which if given without a time limit would be correctly solved by most people. Here it is speed of response that limits performance. Speeded tests will fall somewhere between the two extremes above. There is a time limit that affects the number of items correctly solved and the tests have some difficulty, often gradually increasing difficulty. Tests of maximal performance can be further classified as either norm-referenced or criterion-referenced. With norm-referenced tests, a person’s score (how many items were responded to correctly) is compared with others (known as a norm group) to establish a relative position. With criterion-referenced tests, the concern is with a standard of performance, the presence or absence of a certain level of skill, knowledge or achievement. Such use, sometimes known as mastery testing, involves classification (e.g. pass or fail) or diagnosis (identification of what action needs to be taken). With tests of maximal performance, the behaviour being sampled often appears quite directly related to the construct e.g. the measurement of spatial ability may involve the mental rotation of three-dimensional drawings. 2.1.1.2 Tests of Typical Performance: Normative and Ipsative Tests of personality, interest and integrity (for which there are no ‘correct’ answers) are often described as tests of typical performance (what the person typically does). Such tests commonly consist of items that ask questions about behaviour, beliefs or feelings (e.g. Do you listen carefully when talking with others?), to which a yes /no response is required. Alternatively, the items are statements (e.g. I am often impatient to make my point during conversations), to which either a true/false response is required, or there is a rating scale with 5, 7 or 9 points. Rating scales require an indication of the extent to which the statement is true of the respondent, or the strength of agreement with the statement. Most tests of typical performance measure more than one construct, so that a number of items may be concerned with, for example, conscientiousness, other items may be concerned with openness to ideas, and so on. Regardless of whether the test measures just one construct or many, the response formats so far described allow respondents a free choice of agreeing or not agreeing with each question or statement. Such ‘free choice’ formats are sometimes termed normative measures. A few tests of typical performance contain items that present a forced choice between statements. Here, an item contains more than one statement and there is a requirement to indicate which is most true. Alternatively, there is a requirement to rank order the statements from most to least true. Below is an example showing just two items from a forced choice test: Please indicate which of the following statements is most true of you. For each item tick only one box [ ]. 1. I am almost always on time for appointments [ ] I am very intrigued by new ideas [ ] Once started I usually finish a task [ ] I have varied interests [ ] Note that each item above requires a choice involving more than one construct. The first statement within each item above appears to be concerned with conscientiousness and the second statement appears to be concerned with openness to ideas. Thus, when completing a forced choice measure with many such items, the greater the number of statements that are endorsed for any one construct within the items (e.g. conscientiousness), then the lower must be the number of statements endorsed for the other construct within the items (e.g. openness to ideas). Because of this, it is not possible to score very highly or very lowly on all constructs within forced choice measures. In fact, because every item receives a score, either for one construct or another, the total of scores over all constructs measured will be the same for all respondents; it will just be the distribution of scores between the constructs that will differ. This forced-choice format is sometimes used to reduce socially desirable responding, as with the above example, where both options within an item appear to be equally desirable. Forced-choice measures are sometimes termed ipsative measures. From the above description, their two key features may be summarised as: • The options within each item of the test require choices involving more than one construct • The total score over the constructs being measured will sum to a constant number for all individuals Good ipsative measures are difficult to construct. Also, because a total score is shared out between the constructs, such tests are better suited to measuring the relative strengths of preferences within individuals rather than absolute strengths between individuals (for which normative measures are best suited). With tests of typical performance the behaviour being sampled can appear quite indirectly related to the construct i.e. the behaviour here is responding to statements and questions, perhaps accurately, through the judgement of one’s feelings, behaviours, attitudes, etc. 2.2 Using Tests for Occupational Purposes The main uses for tests within occupational settings may be categorised as: • Part of the selection process for entering an organisation or for promotion within an organisation • Aids for team building, managing change and other organisational issues • Aids to personal development or career guidance Whatever the particular use for the tests, there should always be a clear focus on the assessment needs, and a concern with good practice. The British Psychological Society (the BPS) has a Code of 2.2.1 Good Practice for Psychological Testing http://www.psychtesting.org.uk/the-ptc/guidelinesandinformation.cfm This code outlines what is expected by the BPS when using tests, and includes: Responsibility for Competence e.g. ensuring that test users meet all the standards of competence and endeavour to develop/enhance this competence. Appropriate Procedures and Techniques e.g. ensuring that the test is appropriate for its intended purpose. Storing test materials securely, and ensuring that no unqualified person has access to them. Keeping test results securely, and in a form suitable for developing norms, validation and monitoring bias. Client Welfare e.g. obtaining informed consent of potential test takers, making sure that they understand what tests will be used, what will be done with their results and who will have access to them. Ensuring that test takers are well informed and well prepared for the test session. Providing the test taker and other authorised persons with feedback about the results. The feedback should make clear the implications of the results, and be in a style appropriate to the reader’s/hearer’s level of understanding. Due consideration should be given to factors such as gender, ethnicity, age, disability and specific needs, educational background and level of ability in using and interpreting the results of tests. The particular details for good practice will be derived from the particular purpose for testing. For example, a code of practice for Careers Guidance might include: • The starting point will always be the particular needs of the individual client. • The service provider should be aware of the competencies required within different occupations and endeavour to assess the level of fit between applicant and job, which may include tests of interest, style, motivation and ability • Service providers will be trained to meet the British Psychological Society (BPS) standards for occupational testing • Test users will monitor the extent to which the tests are fair for all • No client will be given an inappropriate or unnecessary test • All clients will have the right to confidential feedback • The client has ownership of the test results and no copies should be kept without prior consent • All materials should be kept securely and with access only by appropriately trained users. 2.3 The Usefulness of Tests in Selection for Jobs Ability tests and personality questionnaires are widely used within employee selection. An older survey by Hodgkinson and Payne (1998) revealed the popularity of these methods for graduate selection is not new. Figure 2.1: Table 1: Selection Procedure usage (percentage of organisations) for Graduates in three European Countries. Adapted from Hodgkinson and Payne (1998). A more recent survey by Zibarras and Woods (2010) revealed the higher use of structured interviews along with ability/aptitiude tests in larger organisations. Importantly, good tests offer reliable and valid measurement. The specific meanings of these two terms will be explored in detail later. For now, we shall describe reliability as consistent measurement and validity as meaningful measurement. One aspect of the meaningfulness of measurement is the extent to which a measure (or score) is related to external criteria (i.e. criterion-related validity) Within occupational testing, this criterion may be job performance. There will of course be many ways of measuring this criterion and thus many potential ways of establishing criterion-related validity. Criterion-related validity is established through obtaining a set of test scores from a large group of people and then obtaining some measure of criterion performance for these same people (e.g. job performance scores). The two sets of scores can then be correlated to give what is known as a criterion-related validity coefficient. Since these validity coefficients are correlation coefficients, their values will range from 0 to 1.00, and are usually symbolised as rxy i.e. the correlation (r) between test scores (x) and criterion scores (y). Table 2 overleaf shows reported criterion-related validity coefficients for a number of selection methods, including general mental ability (GMA), for job performance (from a classic 1998 meta-analysis). Table 2.1: Table 2: Criterion-related Validity for Overall Job Performance of Selection Methods, and GMA in combination with these methods. Adapted From Schmidt &amp; Hunter (1998). Personnel.measures Validity.rxy. Multiple.R Gain.in.validity.from.adding.supplement X..increase.in.Validity GMA tests 0.56 NA NA Integrity tests 0.38 0.67 0.11 20% Conscientiousness tests 0.30 0.65 0.09 16% Employment interviews (all types) 0.35 0.59 0.03 5% Peer ratings 0.36 0.57 0.01 1% Reference checks 0.23 0.61 0.05 9% Job experience (years) 0.01 0.56 0.00 0% Biographical data 0.30 0.56 0.00 0% Years of education 0.20 0.60 0.04 7% Interests 0.18 0.59 0.03 5% A similar table (table 3 below) may be constructed showing the criterion-related validity of methods for training performance. Table 2.2: Table 3: Criterion-related Validity for Training Performance of Selection Methods, and GMA in combination with these methods Adapted From Schmidt &amp; Hunter (1998). Personnel.measures Validity.rxy. Multiple.R Gain.in.validity.from.adding.supplement X..increase.in.Validity GMA tests 0.56 NA NA Integrity tests 0.38 0.67 0.11 20% Conscientiousness tests 0.30 0.65 0.09 16% Employment interviews (all types) 0.35 0.59 0.03 5% Peer ratings 0.36 0.57 0.01 1% Reference checks 0.23 0.61 0.05 9% Job experience (years) 0.01 0.56 0.00 0% Biographical data 0.30 0.56 0.00 0% Years of education 0.20 0.60 0.04 7% Interests 0.18 0.59 0.03 5% The validity coefficient (r_{xy}) for GMA given in table 2 is from an analysis involving over 32,000 employees in 515 diverse jobs (Hunter, 1980; Hunter and Hunter, 1984). This value of 0.51 is actually for ‘medium complexity’ jobs within the sample. Schmidt and Hunter (1998) state that this level of complexity includes 62% of all the jobs in the US economy. This level of job includes ‘skilled blue collar’ jobs and ‘mid-level white collar jobs’ such as upper level clerical and lower level administrative jobs. Within this diversity of ‘medium complexity’ jobs there was reported to be little variation around this value of 0.51. This analysis goes on to report that where validity coefficients did differ, this was for very different levels of job (see table 4 overleaf). Again, within each broad level of job there was reported to be little variation in validity coefficient. Table 2.3: Table 4: How criterion-related validity coefficients vary across broad job- level (from Schmidt and Hunter, 1998) Job.Level Validity.Coefficient Professional managerial 0.58 High-level complex-technical 0.56 Medium complexity 0.51 Semi-skilled 0.40 Completely unskilled 0.23 This analysis suggests that GMA test validity is generally high. Furthermore, the analysis revealed little variation in the validity of ability tests within very broad job families, and thus tests may be said to have considerable validity generalization. The above U.S. work has been supported by more recent European meta-analyses (e.g. Bertua, Anderson &amp; Salgado, 2005; Salgado, Anderson, Moscoso, Silvia, de Fruyt &amp; Rolland, 2003). Salgado et al (2003) report validity for GMA measures across 12 occupational categories in the European Community. They report large validities for job performance and training success in 11 of these categories. Here again, job complexity (low, medium or high) moderated the size of these coefficients. The authors report that these findings are similar to those found in the U.S., although the European data revealed slightly higher validity coefficients in some cases. 2.4 Validity Generalization and Meta-analysis The above picture of generally high and stable validity coefficients is quite different from the general view of ability tests held until the late 1970s. Then, it was generally believed that the criterion-related validity of ability tests was both generally low (i.e. around 0.3) and situationally specific. That is, the validity for a given type of test seemed to be quite different for slightly different job roles and even different locations. However, most of the studies, and especially early studies, used small sample sizes. Because of the small sample sizes involved in any one study, any single study may slightly overestimate or underestimate the ‘real’ relationship between tests and criterion. This is a type of sampling error, which can give rise to seemingly situationally specific validity coefficients. Combining the results of these smaller studies allows the researcher take account of this sampling error and evaluate the extent to which there are real differences between coefficients across job roles and locations. This was the method used by Schmidt and Hunter above which found little ‘real’ variation in coefficients within very broad job families. This method of combining many small studies is termed ’meta-analysis’ and allows for an evaluation of validity generalization. Meta-analysis is often termed validity generalization within psychological testing. Often validity generalization results are also ‘statistically adjusted’ so as to take account of the reliability of the criterion measurement. Low reliability in criterion measurement will lower any correlation between test and criterion. Validity generalization techniques also take account of differences between job applicants and job incumbents. This is because criterion-related coefficients can only be calculated once people are performing the job. Thus, if the test was used within selection, then the coefficient calculated will not include the whole range of test scores that were found within the applicants. This, in turn, will lead to an underestimation of the ‘real’ relationship between test scores and future job performance. See figure 1 for an illustration of this effect. Figure 2.2: Figure 1: An Illustration of the effect of changing the range of test scores on the correlation between test scores and criterion The technique of validity generalisation can upwardly adjust the observed validity coefficients so as to allow for the restricted range of scores found within job incumbents compared to job applicants. This adjustment was used by Schmidt and Hunter to construct the previous tables of validity coefficients and, as we have seen, this gives rise to quite high ability test validity. Cook (1998) suggests that these corrections give ‘estimated mean true validity’ of about twice the mean uncorrected validity. 2.5 Implications from Validity Generalization The technique of validity generalization has revealed that similar tests have quite stable validities for very broad job families. Thus, test users can have some confidence that if a test has evidence of validity for a particular job role, then there is likely to be validity with similar tests in similar job roles. Thus it may be reasonable to assume that test validity information obtained in one situation may generalize to similar situations. Nevertheless, when generalizing from specific validity information, one should check that: • The conditions of the original validity study are similar to the conditions to which one wishes to generalise. For example, a test for the selection of supervisors in an industry with mostly male employees may not show the same validity when used to select supervisors for more mixed industries. • The criterion is similar (e.g. rate of product production may be quite different to supervisor ratings). • The original validation sample size is adequate. • There is a check for restricted range on both test and criterion within the original validation study. • Differential prediction has been considered (i.e. validities may differ for different groups of people). 2.6 Determinants of Job Performance According to Schmidt and Hunter (1992), mental ability has a direct causal impact on the acquisition of job knowledge which in turn leads to higher job performance. They claim that mental ability also has a smaller direct effect upon job performance. These authors also state that job experience, up to about 5 years, operates in the same way, with the major effect of job experience being on job knowledge. Finally, they claim that personality, in particular conscientiousness, also influences job knowledge and thus job performance. Figure 2.3: Figure 2: An Illustration of the suggested relationships between Job Performance and its Determinants (constructed from Schmidt &amp; Hunter, 1992) The above model gives an important role to job experience, up to the first 5 years. Schmidt, Hunter and Outerbridge (1986) report that when experience does not exceed 5 years, the correlation between experience and job performance is around 0.4. These authors report that after 5 years the correlation becomes increasingly weak, with further increases in job experience leading to little improvement in job performance. Thus, as shown previously in table 2 (p.10), the overall validity of job experience for predicting job performance is only 0.18. Table 2 was based on a meta-analysis that included a wide variety of experience, from less than 6 months to more than 30 years. This pattern of relationship, where there is initially a rather strong linear relationship between the predictor and criterion, which then breaks down at the higher end of predictor scores, is termed a twisted pear relationship (Fisher, 1959). Such a relationship is quite common within psychometrics and is illustrated in figure 3. Figure 2.4: Figure 3: An illustration of a twisted pear relationship. Scores on the criterion (Y) and the predictor (X) are quite strongly correlated at the lower end of predictor scores, but there is no correlation at the higher end of predictor scores. 2.7 What is meant by General Mental Ability (GMA) General mental ability is thought to underlie performance on a whole range of ability tests. Spearman (1927) argued that differences in general mental ability are a result of differences in three basic mental processes: the apprehension of experience, the eduction of relations and the eduction of correlates. As an example, consider the simple problem presented below: Ankle is to foot as wrist is to? To solve this analogy, there must first be a perception and understanding of the terms based on past experience (apprehension of experience). Secondly, there must be an inferring of the relationship between the terms (eduction of relationships). Thirdly, there must be an inferring of the missing term so as to satisfy the same relationship (eduction of correlates). For Spearman, then, general mental ability is, in short, the ability to ‘figure things out’. Though this may not seem to give much insight into the meaning of mental ability, this description would satisfy many people currently involved in the field of ability measurement. 2.8 Criterion-related Validity Coefficients and the Coefficient of Determination Criterion-related validity coefficients tell us about the strength of relationship between the test and the criterion, from 0, where there is no relationship, up to 1.00, where there is a perfect relationship between the two sets of scores. A more concrete understanding of this coefficient can be obtained by converting it to what is known as the coefficient of determination. This coefficient of determination is simply the square of the criterion-related validity coefficient, and tells us the proportion of criterion performance that is predicted by the test. Thus, if a validity coefficient (rxy) is calculated as 0.5, then the coefficient of determination (rxy2) will equal 0.25. That is for a validity coefficient of 0.5, then 0.25 (or 25%) of criterion performance is predictable from the test scores. Another way of saying this is that there is 25% overlap between the two sets of scores. Figure 4 below shows the extent of overlap between a set of test scores and a set of criterion scores that correlate at 0.5. Figure 2.5: Figure 4: An illustration of the 25% overlap between test scores and criterion scores when the two sets of scores correlate at 0.5. 2.9 Criterion-Related Validity and Multiple-Correlation It is unlikely that just one test would be used to select people. Many scores can be combined to give the best prediction for any given criterion. The correlation between a criterion and multiple tests (or predictors) can be established using multiple correlation. The multiple correlation coefficient (R) gives an indication of the extent to which adding more tests gives incremental validity The square of the multiple-correlation coefficient (R2) will again give the coefficient of determination. In fact, if just two tests (or predictors) are used, R2 can be calculated directly using equation 1 below: Equation 1: \\[ \\huge R^2_{c.12} = \\frac{r_{c1}^2+r_{c2}^2-2r_{12}r_{c1}r_{c2}} {1-r^2_{12}} \\] Where: \\(R^2\\) is the coefficient of determination for the criterion (c) on the basis of predictors 1 and 2 \\(r_{c1}\\) is the correlation between the criterion and test (predictor) 1 \\(r_{c2}\\) is the correlation between the criterion and test (predictor) 2 \\(r_{12}\\) is the correlation between test 1 and test 2 Consider the following example where: • There is a correlation of 0.51 between the criterion of Job performance and the predictor of General Mental Ability (GMA). • There is also a correlation of 0.54 between the criterion of Job Performance and the predictor of Work Sample (WS). • Finally there is a correlation between the two predictors (GMA and WS) of 0.38. Then: \\[ \\huge R^2_{c.12} = \\frac{.51_{c1}^2+.54_{c2}^2-2\\times .38_{12}.51_{c1}.54_{c2}} {1-.38^2_{12}} \\] \\[ \\huge R^2_{c.12} = 0.4 = 40\\% \\] In this example then, the coefficient of determination for the criterion of Job Performance is 0.4 or 40%. That is, 40% of the variability in job performance is predictable from the two predictors combined. Furthermore, the multiple-correlation coefficient (R) can be determined from this coefficient of determination by simply taking the square root of the coefficient of determination. In the above example: Since \\(\\huge R^2_{c.12} = 0.4 = 40\\%\\) then \\(R_c{12} = \\sqrt{0.4} = 0.63\\). If the reader refers back to Table 2 on page 4, it can be seen that multiple-correlation coefficient given for the combination of GMA and WS in the prediction of Overall Job Performance is in fact 0.63. The original paper from which Table 2 was constructed gives the necessary three terms of equation 1, including the correlation between GMA and Work Sample of 0.38 (not shown on table 2), from which the multiple-correlation may be derived as above. Although equation 1 may look rather complicated, it can be represented in the form of a Venn diagram (Figure 5 overleaf) which illustrates the relationships between correlations as overlapping spaces. Figure 2.6: Figure 5: An illustration of a multiple-correlation between the predictors of General mental ability (GMA) and Work Sample Tests (WM) and the criterion of Overall Job Performance. 2.10 Indicative General References on Selection Beier, M.E., &amp; Oswald, F.L. (2012) Is cognitive ability a liability? A critique and future research agenda on skilled performance. Journal of Experimental Psychology: Applied, 18, 4, 331-345. Bertua, C., Anderson, N. &amp; Salgado, J.F. (2005). The Predictive Validity of Cognitive Ability Tests: A UK Meta-Analysis. Journal of Occupational and Organizational Psychology, 78, 387-409. Bozionelos, N. (2005). When the inferior candidate is offered the job: The selection interview as a political and power game. Human Relations,58(12),1605–1631. Christian, M. S., Edwards, B. D., &amp; Bradley, J. C. (2010). Situational judgment tests: Constructs assessed and a meta-analysis of their criterion-related validities. Personnel Psychology, 63(1), 83–117. Retrieved from http://onlinelibrary.wiley.com/doi/10.1111/j.1744-6570.2009.01163.x/full Cook, M. (2009). Personnel Selection: Adding Value through People (5th ed). Chichester: John Wiley &amp; Sons. Cortina, J. M., Goldstein, N. B., Payne, S. C., Davison, H. K., &amp; Gilliland, S. W. (2000). The incremental validity of interview scores over and above cognitive ability and conscientiousness scores. Personnel Psychology, 53(2), 325–351. Retrieved from http://onlinelibrary.wiley.com/doi/10.1111/j.1744-6570.2000.tb00204.x/abstract Hough, L.M. &amp; Oswald, F.L. (2000). Personnel Selection: Looking Toward the Future – Remembering the Past. Annual Review of Psychology, 51, 631-664. Krause, D.E., Kersting, M., Heggestad, E.D. &amp; Thornton III, G.C. (2006). Incremental Validity of Assessment Center Ratings Over Cognitive Ability Tests: A Study at the Executive Management Level. International Journal of Selection and Assessment, 14, 360-371. Kuncel, N.R. &amp; Hezlett, S.A. (2010) Fact and Fiction in Cognitive Ability Testing for Admissions and Hiring Decisions. Current Directions in Psychological Science, 19, 6, 339-345 http://intl-cdp.sagepub.com/content/19/6/339.full Kuncel, N.R., Ones, D.S., &amp; Sackett, P.R. (2010). Individual differences as predictors of work, educational, and broad life outcomes. Personality and Individual Differences, 49, 4, 331-336. http://www.sciencedirect.com/science/article/pii/S0191886910001765 Lievens, F., &amp; Patterson, F. (2011). The validity and incremental validity of knowledge tests, low-fidelity simulations, and high-fidelity simulations for predicting job performance in advanced-level high-stakes selection. Journal of Applied Psychology, 96(5), 927–940. https://doi.org/10.1037/a0023496 Macan, T. (2009). The employment interview: A review of current studies and directions for future research. Human Resource Management Review, 19(3), 203–218. https://doi.org/10.1016/j.hrmr.2009.03.006 Piotrowski, C. &amp; Armstrong, T. (2006). Current Recruitment and Selection Practices: A National Survey of Fortune 1000 Firms. North American Journal of Psychology, 8, 489-496. Ployhart, R.E. (2006). Staffing in the 21st Century: New Challenges and Strategic Opportunities. Journal of Management, 32(6), 868-897. Robertson, I.T. &amp; Smith, M. (2001). Personnel Selection. Journal of Occupational and Occupational Psychology, 74, 441-472. Rothstein, M. G., &amp; Goffin, R. D. (2006). The use of personality measures in personnel selection: What does current research support? Human Resource Management Review, 16(2), 155–180. https://doi.org/10.1016/j.hrmr.2006.03.004 Ryan, A. M., &amp; Ployhart, R. E. (2014). A Century of Selection. Annual Review of Psychology, 65(1), 693–717. https://doi.org/10.1146/annurev-psych-010213-115134 Sackett, P.R., &amp; Lievens, F. (2008). Personnel selection. Annual Review of Psychology, 59, 419-50. http://www.numerons.in/files/documents/Personnel-Selection-Paul-R.-Sackett-and-Filip-Lievens.pdf Salgado, J.F., Anderson, N., Moscoso, S., Silvia, B.C., de Fruyt, F. &amp; Rolland, J.P. (2003). A Meta-Analytic Study of General Mental Ability Validity for Different Occupations in the European Community. Journal of Applied Psychology, 88, 1068-1081. Salgado, J. F. (2016). A Theoretical Model of Psychometric Effects of Faking on Assessment Procedures: Empirical findings and implications for personality at work. International Journal of Selection and Assessment, 24(3), 209–228. Retrieved from http://onlinelibrary.wiley.com/doi/10.1111/ijsa.12142/full Schmitt, N. (2014). Personality and Cognitive Ability as Predictors of Effective Performance at Work. Annual Review of Organizational Psychology and Organizational Behavior, 1(1), 45–65. https://doi.org/10.1146/annurev-orgpsych-031413-091255 Schmidt, F. (2006). The Orphan Area for Meta-Analysis: Personnel Selection Retrieved October 30, 2016, from http://www.siop.org/tip/Oct06/05schmidt.aspx Schmidt, F.L. &amp; Hunter, J.E. (1998). The Validity and Utility of Selection Methods in Personnel Psychology: Practical and Theoretical Implications of 85 Years of Research Findings. Psychological Bulletin, 124, 262-274. Schmidt, F.L. &amp; Hunter, J.E. (2004). General Mental Ability in the World of Work: Occupational Attainment and Job Performance. Journal of Personality and Social Psychology, 86,162-173. Scior, K., Bradley, C. E., Potts, H. W. W., Woolf, K., &amp; de C Williams, A. C. (2014). What predicts performance during clinical psychology training? British Journal of Clinical Psychology, 53(2), 194–212. https://doi.org/10.1111/bjc.12035 Scroggins, W. A., Thomas, S. L., &amp; Morris, J. A. (2008). Psychological testing in personnel selection, part II: The refinement of methods and standards in employee selection. Public Personnel Management, 37(2), 185–198. Retrieved from http://ppm.sagepub.com/content/37/2/185.short Scroggins, W. A., Thomas, S. L., &amp; Morris, J. A. (2009). Psychological testing in personnel selection, part III: The resurgence of personality testing. Public Personnel Management, 38(1), 67–77. Retrieved from http://ppm.sagepub.com/content/38/1/67.short Smith, M. &amp; Smith, P. (2005). Testing People at Work: Competencies in Psychometric Testing. Oxford: BPS Blackwell. Zibarras, L.D., &amp; Woods, S.A. (2010). A Survey of UK Selection Practices Across Different Organization Sizes and Industry Sectors. Journal of Occupational and Organizational Psychology, 83, 499-511. "],["NDSM.html", "Chapter 3 The Normal Distribution and scales of Measurement 3.1 Percentiles 3.2 Standard scores (z) 3.3 Normalising z scores 3.4 Standardised scores", " Chapter 3 The Normal Distribution and scales of Measurement 3.1 Percentiles The above figure showed the relationship between the normal distribution curve for a set of scores in a norm group and scales commonly used within tests. When giving test feedback it is most common to use the percentile scale since this is most readily understood e.g. you have scored higher than X% of the comparison group. One problem with this percentile scale is that it exaggerates the difference in raw scores at the middle of the distribution and minimises differences at the extremes. For example, the difference in raw scores between the 50th and 60th percentile will be much smaller than the difference in raw scores between the 80th and 90th percentile (note how the percentile change is not equal as you move up equal intervals on the horizontal axis of the normal distribution). Thus the percentile scale does not represent equal intervals between raw scores. Other scoring systems have been employed which, along with other advantages, maintain the interval relationship between the raw scores. These scales are discussed below. 3.2 Standard scores (z) z scores are usually termed standard scores. These standard (z) scores express the difference from the mean in standard deviation units as shown below: \\[ z = \\frac{(score_{raw} – mean_{norm})}{SD_{norm}}\\] By this method tests that have a normal distribution of scores, but have differing means and standard deviations, may be converted to a common comparison scale with a mean score of zero (0) and a standard deviation of one (1). For example, a raw score that is exactly 2 standard deviations above the mean for a test gives a z score of 2. From the normal distribution curve (NDC) table we know that a z score of 2 corresponds to the 98th percentile. Furthermore, from this NDC table we know the percentile equivalent for any given z score. Thus we can meaningfully compare performance across tests so long as the test scores are normally distributed and so long as the tests involve the same norm groups. The reasons for these two conditions are given below: Firstly, if test scores are skewed from the normal distribution then z scores derived from the above formula will also be skewed and the NDC table will no longer give accurate percentiles for the test. The standard score formula retains the relative distances between scores found in the original raw scores. For example, consider four raw scores: 55, 60, 70 and 80 for a test with a mean of 50 and a standard deviation of 10. The first two scores differ by 5 points and the last two differ by 10 points – twice the difference of the first pair. When converted to z scores these raw scores give values of: 0.5, 1.00, 2.00 and 3.00. Thus, the first two scores differ by 0.5 z scores and the last two differ by 1.00 z scores – twice the difference of the first pair. Thus the conversion of raw scores to z scores using the above formula may be described as a linear transformation i.e. where the raw score distribution shape is maintained in the standard scores. If raw scores are skewed then the corresponding linearly transformed z scores will also be skewed, and the NDC table will underestimate (if scores are positively skewed) or overestimate (if scores are negatively skewed) the actual percentile score. Secondly, to meaningfully compare performance across tests we need to ensure that the norm groups across the test are in fact comparable e.g. it makes little sense to compare two different tests using two different norm groups. Consider comparing verbal performance using an A Level norm group with numerical performance using a first form norm group. Here, any difference in percentile for a person taking both tests could come about either as a result of a real difference in ability or as a result of norm group differences. 3.3 Normalising z scores To address the problem of comparability across tests when test scores are skewed, some tests ‘normalise’ the z scores for the set of raw scores. This is a non-linear transformation. This can be done by first obtaining the percentile equivalent for each raw score (this can be worked out directly or found within the norm table). Next, for each percentile find the corresponding z score from within the NDC table. By this process raw scores are transformed into a normal distribution of z scores known as normalised standard scores. This process is usually considered justified when the underlying attribute being measured is believed to be normally distributed and the non-normality is considered to be a result of sampling error or error in test construction. 3.4 Standardised scores Apart from z scores and percentiles there are a number of other scoring systems that are often collectively termed standardised scores. The relationship between these standardised scores and z scores can be simply described using the following formula: \\[ Score_{scale} = (SD_{scale} * Score_z) + Mean_{scale}\\] For example, the T scale scoring system is defined as having a mean of 50 and a standard deviation of 10. Thus, if we have a z score of -1, the corresponding T score will be: \\[T score = (10*-1) + 50\\] \\[= (-10) + 50\\] \\[= 40\\] Thus, to convert a z score to any other standard scoring scale we need to know the scale’s mean and standard deviation. These standardised scores retain the distribution of the original raw scores if the z score used in the above formula is itself a linear transformation (i.e. a non-normalised z score). In this case, such scores may be described as linear standardised scores (e.g. linear T, etc). If, however, the z score has been normalised then score may be described as a normalised standardised score (e.g. normalised T, etc). Below are some commonly used standardised scores: T scores have a mean of 50 and a standard deviation of 10 IQ or intelligence test scores usually have a mean of 100 and a standard deviation of 15. Sten scores have a mean of 5.5 and a standard deviation of 2. Stanine scores have a mean of 5 and a standard deviation of 2. A knowledge of these scales allow us to work from raw score to standardised score or vice versa. For example, if a test has a mean raw score of 80 and a raw score standard deviation of 10, then the linear T score equivalent to a raw score of 95 may be calculated as follows: \\[ z = \\frac{x-\\bar{x}}{sd} = \\frac{95-80}{10} = 1.5 \\] \\[ T = \\bar{x_t}+(sd_T \\times z) = T 50 + (10 \\times z) = 50 + (10 \\times 1.5) = 65 \\] And, if on the same test (with raw score mean of 80 and raw score SD of 10, then if a person has a linear T score of 70, then the corresponding raw score may be calculated as follows: Since \\(T=\\bar{x_T} + (sd_T \\times z)\\), therefore \\(z = sd= \\frac{T-\\bar{X_T}}{sd_T}, z = \\frac{70-50}{10}, z = 2\\), , Then, since \\(z = \\frac{x-\\bar{x}}{sd}\\), therefore \\(x = (sd \\times z) + \\bar{x}, x = (10 \\times 2) + 80\\) So a T score of 70 here equals a raw score of 100. "],["reliability.html", "Chapter 4 The Reliability of Measurement 4.1 Reliability 4.2 Reliability and Classical Test Theory 4.3 Assumptions of Internal Consistency Methods 4.4 Generalisability Theory 4.5 Using Reliability coefficients to Calculate Confidence Intervals 4.6 Calculating the Standard Error of Measurement (SEM) 4.7 Indicative Reading", " Chapter 4 The Reliability of Measurement Tests typically contain many items for measuring a construct. As has previously been mentioned, these items are a sampling of relevant questions/items from the much greater number of items that potentially could be used to measure the construct. A number of important questions arise from this: how many items should be included in the test, how diverse should they be, and how can we tell if they have a common core? These questions, as we shall see, are about building a test that gives consistent or reliable measurement. Reliability enables meaningful measurement. Furthermore, even if we can answer these questions, how can we know if the test is any good for its intended use, that is, whether the test is measuring something meaningful and useful? This is a matter of establishing the validity of measurement. 4.1 Reliability Firstly, to address how many items should be included on a test and how diverse should they be. Take, for example, a test intended to measure mathematical reasoning. If the test is to measure reasoning then its items should involve reasoning or ‘figuring things out’, and for a test of mathematical reasoning the item content should have a mathematical content or relevance. So far, this is obvious, but there are a many sorts of problems that involve reasoning. These may include: Figuring out analogies such as: 60 is to 20 as 120 is to? Figuring out the odd term such as: 362, 144, 313, 242 Figuring out the sequence such as: 7, 10, 16, 28… Thus, there are very many ways in which to present a reasoning problem, and within each way there are many different mathematical operations that could be included (e.g. just addition, just subtraction, just division, just multiplication or any combination of these operations). Why not just take one way of presenting the problem and one kind a mathematical operation, perhaps using just a few items on the test? This would be a very poor measure because: 4.1.1 Few items will not represent the whole domain of the construct For example, consider a test of mathematical reasoning that just included items of sequences involving multiplication. Unless, for everybody, this type of content is very highly correlated with other ways of mathematical reasoning, we will not have measured all that we mean by mathematical reasoning. Thus we should include items from the whole range of possible item content so as to represent the breadth of the construct we intend to measure; the broader the construct the more items we will need. 4.1.2 Difficulty levels cannot be properly varied with few items and so cannot reveal a range a difference between people. Thus, the more items there are on a test and the more varied these are in difficulty, then the more potentially discriminating the test will be. Furthermore, any single item is likely to be a poor measure because of the various sources of random error that can affect performance on one item. Such sources include: 4.1.3 Any one item may be poorly constructed Any specific item may be a poor measure because it is badly constructed. For example, it may be just too easy or difficult or it may include irrelevant content. Thus, the more items there are on a test, the less influence there will be from one or two poor items. 4.1.4 Any one item may offer advantage or disadvantage to the test taker because of their previous experience For any specific item, an individual may advantaged or disadvantaged by their previous level of familiarity with just this type of reasoning, and/or just this type of content. The item may, for example, just hit a ‘blind spot’. Thus, the more diverse the items are, the more likely it is that such influences will balance out. 4.1.5 Any one item may be affected by very temporary events When attempting to answer any single item the person may be affected by external distractions (e.g. noise, the person next to them, etc) or internal distractions (e.g. suddenly wondering if they left the iron on). Lucky guesses could also be placed in this category. Thus the longer the test the less influence very temporary events will have on the total test score. To give reliable or consistent measurement, a test thus generally needs many diverse items to measure a construct so as to balance out the various sources of random error outlined above. 4.2 Reliability and Classical Test Theory The above discussion of the need for many diverse items is reflected in Classical Test Theory. This states that the response to any item within a test can be thought of as being made up of two contributing influences: the response is partly due to the person’s true ability or personality (or whatever construct is being measured) and partly due to the various sources of random error outlined previously. That is: \\(Item Score (X) = True Score + Random Error\\) If this is the case, then by adding together lots of item scores the random error will increasingly cancel itself out and the total score across the items will increasingly reflect a person’s true score. That is: Total Score across Items (X) = True Score (and more true score than at an item level) + Random Error (and less random error than at an item level) The True score (T) is the score the person would obtain if they were tested using all of the possible items from the relevant construct domain. As such it is a hypothetical score. Clearly, to be representative, broad constructs will need more items than narrow constructs. Furthermore, because this usually involves sampling many items, any random errors within individual items are likely to balance out over the whole collection of items within the test. However, this diversity of item content will only increase accuracy of construct measurement if the items falls within the construct domain (e.g. mathematical reasoning), that is, if there is a common core to the items. 4.2.1 Determining the Reliability Coefficients In general then, the more items a test contains, the more reliable will be the measurement i.e. the less error there will be. However, good items are not easy to write, and of course the practical use of the test means that they should not take too long to complete. Kline (1993) claims that a 10 item test can give quite consistent measurement, and a 20 item test can give very satisfactory reliability. These are very much rough guides, to know with any confidence we should determine the reliability of the test, and for this we need to calculate a reliability coefficient. Classical reliability theory shows that a test’s reliability is equal to the average inter-correlation of all possible same length tests that sample the same item content. This average inter-correlation gives the reliability coefficient (\\(r_{xx}\\)) for the test. Perhaps the ideal way of calculating a reliability coefficient would be: • To construct many parallel versions of a test (i.e. many same length tests that each sample items from the same construct domain) • Have a very large group of people (i.e. a norm group) take each parallel version, for example everyone takes one version of the test every week for a month • Correlate the norm group test scores between every test and calculate an average correlation coefficient between the tests Such an average correlation coefficient would give a very useful index of the extent to which test scores change due to item sampling (item sampling error) and due to time of testing (time sampling error) The reliability coefficient can be thought of as representing the proportion of true score variance (\\(\\sigma^2t\\)) to observed score variance (\\(\\sigma^{2}x\\)). Thus, if we have a reliability coefficient of 0.8 then 0.8 or 80% of the tests variance is attributable to true score variance and 20% is due to random error. Thus the greater the reliability coefficient then the more the scores on a test will reflect ‘true’ scores, and the more consistent will be the measurement. Calculating the reliability coefficient would involve inter-correlating the scores from all possible same length tests from the relevant construct domain. In practice, reliability coefficients are estimated from the inter-correlation of some of the tests from the relevant construct domain. These practical methods are listed below: Test-Retest methods Alternate Form methods Split-half methods Internal Consistency methods Scorer Reliability methods Each of these methods gives information on the reliability of measurement, although what is meant by the term consistency is rather different in each case. 4.2.2 Test-Retest Methods On two occasions, give the test to a large sample (n &gt; 100) of people for whom the test is intended; score the test for the two occasions, then correlate the two sets of scores. If the sample has very similar test scores on the two occasions, the correlation coefficient will be high, and there is evidence for consistency of measurement over time. This method has the advantage of being conceptually simple, but there are some difficulties, for example: 4.2.2.1 How long a gap between test and re-test? If it is too short the respondents may just remember their previous responses. Kline (1993) recommends a gap of at least three months between retests. If, however, it is too long, respondents may actually have changed with regard to the construct being measured. This is especially likely to be the case for children rather than older people. Anastasi (1997) suggests that, for any group of respondents, the interval between retests should rarely exceed six months. In all cases the test manual should state the interval used. 4.2.2.2 Which constructs are suitable for test-retest methods? Measures of attitude and current state (e.g. anxiety) may be less suitable than measures of personality and ability. Kline (1993) suggests that a correlation of 0.8 is a minimum figure for test-retest reliabilities. 4.2.3 Alternate Form Methods (Parallel form methods) Here, it is necessary to construct different versions of the same test (i.e. tests the have same content domain, same mean and same variance). Then, on two occasions, give different versions of the same test to a large sample (n &gt; 100) of people for whom the test is intended, and then correlate the two sets of scores. This method avoids the difficulty of people simply remembering their previous responses, and there can therefore be small time gaps between administrations. It is, however, difficult and expensive to properly construct alternate forms, and there are few available. Also, there still may be a problem with general practice or sensitising effects. This would not be a problem if everybody “benefits” to the same extent (such systematic bias will not affect the correlation), however it is more usual for some people to benefit more than others. 4.2.4 Split-half Methods Here, the correlation is between two halves of one test. This has the advantage of only needing one testing session and only one test. However, there is the problem of how to split the test in two for purposes of correlation. Furthermore, the obtained correlation is for a test that is half as long as the actual test, and as such will be an underestimation of the reliability of the test. The Spearman-Brown formula may be used to correct for this underestimation: \\(r_{est} = \\frac{2r_{ob}}{1+r_{ob}}\\) Where: \\(r_{est} =\\) estimated correlation of full test \\(r_{ob}\\) = obtained correlation from the split half The above formula is actually an application of the more general formula for estimating the effect of lengthening a test \\(\\frac{r’=n r_{xx}}{1+(n-1 r_{xx})}\\) Where: \\(r’\\) = estimated reliability of changed test n = factor by which the test length is increased \\(r_{xx}\\) = reliability of the original test 4.2.5 Internal Consistency Methods These methods estimate the reliability of the test from the number of items in the test and the average inter-correlation among the items. Such internal consistency estimates can be shown to be equivalent to the mean of all possible corrected split-halves for a test of a given length. This means that internal consistency methods give the average correlation between all possible same length versions of the test content. As such, internal consistency methods are considered to be the best estimate of the theoretical correlation between all possible pairs of tests in the domain, however such methods do not take account of error due to time of testing (i.e. time sampling error). The formulas for calculating internal consistency take into account the number of items on the test (i.e. length of the test); because the more items there are on a test then the more likely it is to balance out the influence of random error. However, this will only be the case if the items actually share a common core and therefore the formulas also take into account the extent to which there is some consistency of responses between the items by using the average inter-item correlation. Coefficient alpha is used for calculating internal consistency when items have more than two response options (e.g. personality scales) \\[ r_{xx} = \\frac{n \\times r_{averagejj}}{1+ (n-1) r_{averagejj}} \\] Where: \\(r_{xx}\\) = reliability of the test (internal consistency) n = number of items on test \\(r_{average}jj\\) = average inter-correlation of the test items The more conventional way of calculating coefficient alpha (which also takes account of any differences in standard deviations across items) is shown below: \\(r_{xx} = \\frac{n}{n-1} (1-\\frac{\\Sigma sd^2_i}{sd^2x_i})\\) Where: \\(r_{xx}\\) = coefficient alpha n = number of items on the test \\(∑sd2i\\) = the sum of the item variances \\(sd2x\\) = the variance of scores on the total test Kuder-Richardson formula 20 (KR-20) is used when items are scored 0 or 1 (e.g. right or wrong): \\(r_{xx} = \\frac{n}{n-1}\\times\\frac{1-\\Sigma p_q}{sd^2_x)}\\) Where: \\(r_{xx}\\) = KR-20 n = number of items on the test p = proportion of test takers getting each item right q = proportion of test takers getting each item wrong \\(sd^2_x\\) = the variance of scores on the total test An important implication of these formulas, is that for two tests of the same length, the more reliable test must have greater variation in the scores across test takers and so be more discriminating between respondents. As a general rule, internal consistency coefficients should not drop below 0.7, and ideally be around 0.9 or even higher if used to make important decisions about individuals. 4.2.6 Reliability and the Effect of Combining Scores from Separate Tests The higher the correlation between two tests or scales, then the higher will be the reliability of two scores combined from these scales. Thus composite scores are typically more reliable than the individual tests, and their reliability can be calculated from the correlation between the tests and the average test reliability. Furthermore, the higher the correlation between two scales then the lower will be the reliability of the difference score. Difference scores reflect differences in true scores and differences due to measurement error. With highly correlated scales there will be little difference in the true scores (since both are measuring the same construct), thus any difference between two scale scores is likely to be due to measurement error. The reliability of difference scores can be calculated from the reliability of the two tests and their correlation. 4.3 Assumptions of Internal Consistency Methods In the same way that we need to check assumptions when running statistical analyses on our data, we also need to check assumptions when establishing the internal consistency of our measures. Whilst Cronbach’s Alpha is not the most conservative of measures of internal consistency, it is still more restrictive than we might like. There are three reliability models, each with progressively fewer restrictions. 4.3.1 Unidimensionality First and foremost, any time we use a measure, we need to ensure that it is Uni-dimensional, that is only measuring a single factor, rather than a series of subscales. This is obvious when thinking about tests - we would not purposefully include verbal intelligence items in a spatial reasoning task - but less so when considering inventories such as Portrait-Values or Personality measures where questions might be ambiguous, or relate to many different constructs. In this regard it is always important to check for dimensionality when using inventory measures. Common ways to do this are exploratory factor analysis and parallel analysis. 4.3.2 Reliability Models Unidimensionality is common to all reliability models. If a measure is formed of subscales, then these ought to be checked separately. After Unidimensionality has been established, the model that is selected will depend on the structure of the factor and the features of the items. 4.3.2.1 Parallel Parallel reliability models are the most restrictive. They assume that not only do items have the same covariances, i.e. they relate to each other in the same way, but also that they have the same variance. We saw earlier that parallel forms reliability requires sets of items that are ostensibly identical in all but wording, and this is what that would look like. This is perhaps easier to achieve when making tests: 1 + 2, and 2 + 1 are parallel in that they have the same elements, the same answer, but are just rearranged. When making inventories however, it is much more difficult: can we really say that two items are identical if they mention different objects? Take for instance ‘I generally vote for liberal candidates’ and ‘I generally vote for conservative candidates’ - to say these are parallel would imply that the correlation between them was about -1, i.e. a person who scores 1 on the former will always score 5 on the latter - but that is unlikely since there will be variation in how people interpret the words conservative or liberal. These may have asymmetric meanings - a conservative might vote for a left leaning candidate under some circumstances, whereas a liberal voter might never vote for a conservative regardless of the alternative. 4.3.2.2 Tau/ Esesentially Tau Equivalent Tau equivalnce allows the variances of items to change, but assumes that the covariances remain the same - that is, were the population to be sampled, the covariances between items would be identical (so sample covariances should be similar, give or take sampling error). This is perhaps a little too much to ask, and so Essential Tau equivalence, where each item is assumed to have a common true score, is used instead. 4.3.2.3 Congeneric Congeneric reliability is the least restrictive model of reliability. It literaly means ‘of the same genus’, so rather than only measuring lions, one could also measure attitudes towards tigers, cheetahs, and Bobcats too. Congeneric reliability allows both variances and covariances of items to vary. They still need to be correlated with the same factor, but the strength of correlation is free to vary. 4.4 Generalisability Theory Classical reliability theory considers inconsistencies in test scores to be random error, and the different sources of random error are identified by the various methods for establishing reliability. In contrast, generalisability theory attempts to identify specific and systematic sources of inconsistency (e.g. using a test for selection compared to using the same test for career development). Here the concern is with the extent to which test scores from one situation are generalisable to scores in other situations. Within generalisability theory, whether a test is reliable depends upon the conditions of measurement and the intended use of the test scores For example, if we wish to generalise a test score to how people perform under pressure then testing people with harsh supervisors is not a source of random error. 4.5 Using Reliability coefficients to Calculate Confidence Intervals As discussed previously, classical test theory states that any measurement can be thought of as being made up of two parts: the person’s true score for that attribute and some error component. The True score (T) is the score the person would obtain if they were tested using all of the possible items from the relevant content domain (i.e. no content sampling error). As such it is a hypothetical score. In this theory all important errors in measurement are considered to be random and normally distributed. Thus if a person were to be given a subset of test items repeatedly, and if we were able to achieve no carry-over effects from one test session to the next, then we would expect a distribution of obtained scores the mean of which would approximate the person’s true score. This is obviously a hypothetical situation. Clearly, the more random error associated with a particular test then the greater will be the variance (and standard deviation) of this hypothetical distribution. Although it is impossible to actually repeatedly test an individual in such a way, the standard deviation of this hypothetical distribution can be estimated from knowledge of the reliability coefficient for the test and the standard deviation of obtained scores for a population of test takers (i.e. the norm group). This estimate of the standard deviation for the above distribution is termed the Standard Error of Measurement (SEM). 4.6 Calculating the Standard Error of Measurement (SEM) The SEM may be calculated as follows: \\[\\large SEM = SD\\sqrt{1-r_{xx})}\\] Where: SD is the standard deviation for the test scores obtained by the norm group (or the SD of transformed scores e.g. T scores) and \\(r_{xx}\\) is the reliability coefficient for this same group. The SEM is a useful statistic because it allows us to put concrete confidence bands around any observed score. For example, if the obtained ability raw score of a person is, for example, 100, and the SEM for these raw scores was 5, then we could be 95% confident that his or her true score on this test is likely to fall within 9.8 (i.e. 1.96 x 5) points of this obtained score i.e. the person’s true score is likely to be between 90.2 and 109.8 (i.e. 100 - 9.8 to 100 + 9.8). The logic here is as follows: if, as discussed above, 95% of observed scores will fall within about 2 SEM (1.96 SEM to be precise) of the true score, then 2 SEM around any given observed score will contain the true score on 95% of occasions. The figure below illustrates a case where the test SEM is 5 points and there is an observed score of 100. The score of 100 is obviously within the 95% confidence band for a true score of 100, but it is also within the 95% confidence band for a true score of 90 and for a true score of 110. Thus an observed score of 100 may come from a true score anywhere between 90 and 110, and this will be the case on 95% of occasions. When using the Standard error of measurement we are constructing confidence bands on the basis of the likely test score range for any given true score. In the last example, a 95% confidence band was calculated. By using normal distribution curve (NDC) tables it is possible to calculate any level of confidence band for a given score. Norm tables within test manuals often give 68% confidence bands for any obtained score (i.e. 1 SEM around the observed score). . 4.6.0.1 Calculating the Standard Error of Difference (SEdiff) As well as placing confidence bands around individual scores, reliability coefficients may also be used to assess confidence about differences in test score. For example, if a person scores differently on two tests, how confident can we be that this is a real difference in performance rather than just measurement error associated with both tests? A similar question is: if two people score differently on the same test, how confident can we be that they actually do differ in underlying ability rather than this being the result of error within the test? To answer these questions we can use the Standard error of Difference (SEdiff) as follows: \\[ \\large SEdiff = \\sqrt{(SEM^2_{test1} + SEM^2_{test2})} \\] The standard error of difference can be seen as the standard deviation of the distribution of differences scores, and involves the SEM of both tests. These two tests could either be the same test on two different people (in which case the SEM above will be the same on both tests) or it could be two different tests on the same person (in which case the SEM above will be different for test 1 compared to test 2). In either case, the application of the above formula requires that test 1 and 2 are using a comparable measurement scale i.e. the tests have the same mean and standard deviation. This means that the above formula may be rewritten as shown below: \\[ \\large SEdiff = sd\\sqrt{(2 – r_{xxtest1} - r_{xxtest2}}\\] If we were comparing performance on two different tests we must use one of the standardised scoring systems. If we used the T scoring system, this would give both tests a standard deviation of 10. Then if, for example, test 1 has a reliability of 0.75 and test 2 has a reliability of 0.85, then the SEdiff may be calculated as follows: \\(SEdiff = 10\\sqrt{(2 - 0.75 - 0.85)}\\) \\(= 10 \\times 0.63\\) \\(= 6.3\\) Thus, since the SEdiff is a unit of standard deviation, we can be 68% confident that if the two test scores differ by as much as 6 T score points (some rounding is need here) then there is a reliable difference in test score performance. If the two scores differ by as much as 12 T score points (i.e. 1.96 x SEdiff) then we can be 95% confident that there is a reliable difference in performance. When comparing people on the same test we could use the raw score standard deviation in the calculation of the SEdiff or we could use one of the standardised scoring systems as above (e.g. T scores), In fact, when the same test is involved the SEdiff formula reduces to that shown below: \\(SEdiff = \\sqrt{(2)} \\times SEM\\) \\(SEdiff = 1.414 \\times SEM\\) If, for example, we were using T scores as the basis for comparison and the test had a reliability of 0.8 (a fairly typical level) then the SEdiff would be: \\(SEdiff = 1.414 \\times (10\\sqrt{1-0.8})\\) \\(= 1.414 \\times 4.47\\) \\(= 6.3\\) The interpretation of the above statistic is the same as the previous example. 4.7 Indicative Reading Charter, R.A. (1997). Confidence Interval Procedures for Retest, Alternate-Forms, Validity and Alpha Coefficients. Perceptual and Motor Skills, 84, 1488-1490. Charter, R.A. &amp; Feldt, L.S. (2000). The Relationship Between Two Methods of Evaluating an Examinee’s Difference Scores. Journal of Psychoeducational Assessment, 18, 125-142. Charter, R.A. &amp; Feldt, L.S. (2001). Confidence Intervals for True Scores: Is there a Correct Approach. Journal of Psychoeducational Assessment, 19, 350-364. Charter, R.A. &amp; Feldt, L.S. (2002). The Importance of Reliability as it Relates to True Score Confidence Intervals. Measurement and Evaluation in Counselling and Development, 35, 104-112. Charter, R.A. &amp; Feldt, L.S. (2009). A Comprehensive Approach to the Interpretation of Difference Scores. Applied Neuropsychology, 16, 23-30. Charter, R.A. (2009). Differences Scores: Regression-Based Reliable Difference and the Regression-Based Confidence Interval. Journal of Clinical Psychology, 65(4), 456-460. Cho, E. (2016). Making Reliability Reliable: A Systematic Approach to Reliability Coefficients. Organizational Research Methods, 19(4), 651–682. https://doi.org/10.1177/1094428116656239 Dudek, F.J. (1979). The Continuing Misrepresentation of the Standard Error of Measurement. Psychological Bulletin, 86, 335-337. Glutting, J.J., McDermott, P.A., &amp; Stanley, J.C. (1987). Resolving Differences among Methods of Establishing Confidence Limits for Test Scores. Educational and Psychological Measurement, 47, 607-614. Helms, J.E., Henze, K.T., Sass, T.L., Mifsud, V.A. (2006). Treating Cronbach’s Alpha Reliability as Data in Counseling Research. The Counseling Psychologist, 34, 630-660. Harvill, L. M. (1991). Standard Error of Measurement. Educational Measurement: Issues and Practice, 10, 33–41. doi:10.1111/j.1745-3992.1991.tb00195.x Standard Error of Measurement - NCME Within the above link: you have used equation 6 in the article for the SEM within your data record sheets, and equation 10 for the SEdiff within your data record sheets (though article calls this the SEM for the score differences) Mortensen, E. L., &amp; Gade, A. (1992). Linear versus normalized T scores as standardized neuropsychological test scores. Scandinavian Journal of Psychology, 33(3), 230–237. Tavakol, M., &amp; Dennick, R. (2011). Making sense of Cronbach’s alpha. International Journal of Medical Education, 2, 53–55. "],["validity.html", "Chapter 5 The validity of measurement 5.1 The relationship between reliability and validity:", " Chapter 5 The validity of measurement The previous sections on the reliability of measurement were concerned with the issue of consistency of measurement, and addressed such questions as how broad is the construct we wish to measure and how many items are needed to give reliable measurement of the construct. Answering these questions involved reliability analysis. Such analysis gives test constructors and test users some confidence that the test is giving a dependable measure with little error in measurement. Thus, meaningful measurement is a possibility. However, there is still a question of whether the test is any good for its intended use, that is, whether the test is measuring something appropriate, meaningful and useful. Many textbooks describe this as establishing whether the test actually measures what it claims to measure and this is termed the validity of measurement. Traditionally, issues of validity are considered under three categories of evidence: Content validity Criterion related validity Construct validity A distinction needs to be drawn between these categories of evidence and what the test merely appears to be for test users and test takers. The extent to which the test appears to be measuring what it claims to measure, without reference to any evidence from outside of the test itself, can be described as face validity. Thus, the instructions for the test, the practice items and main test content may all appear to be relevant to the construct the test claims to measure. This impression can sometimes be misleading, but it may be useful for the test candidate to see the apparent relevance of the test. In contrast to face validity, the three categories of validity evidence link the test to sources of evidence outside of the test itself. 5.0.1 Content validity This is the extent to which the test content accurately reflects the content domain of the construct. This is easier to achieve for tests of achievement rather than tests of more abstract constructs such as spatial reasoning. Nevertheless, a test of spatial ability, for example, should probably include items involving the folding, rotation and comparison of complex visual images. Beyond this face validity, the test user should consult ‘expert sources’ to verify the content validity of the test e.g. the relevant academic literature, existing tests and people working in relevant occupations. 5.0.2 Criterion-related validity This is the extent to which the test is related to external criteria (e.g. academic achievement for a test of intelligence or line manager ratings of job performance for a test of clerical ability). This is established through correlation between the test and the criterion. The relevance of the criteria depends upon the purposes of the test. A test of spatial ability, for example, may be expected to correlate with training and/or job performance for people within the engineering and construction industries. The criterion measures used to validate a test may be gathered at about the same time as the test scores or after a stated interval. This is the usual distinction between concurrent (same time) and predictive (with interval) validation. 5.0.2.1 Predictive validation This is usually involved when attempting to predict future behaviour from current test score. Typically, this is for selection purposes (e.g. job applicants, college admissions, assignment to training programmes, etc). In its purest form, predictive validity involves obtaining test scores from applicants but not using these to make decisions; then at some later time correlating these earlier test scores with performance or outcome scores. 5.0.2.2 Concurrent Validation By contrast concurrent validation is usually involved when attempting to assess current status/condition. Within an employment context this would mean that the test scores are correlated against performance measures for an already selected group. 5.0.2.3 What are the advantages and disadvantages of each procedure, and how do selection effects impact on the outcome of the procedures? In the purest form predictive validation has the advantage of providing a direct measure of the actual relationship between test scores and future performance for the general applicant pool. However, there are practical and ethical difficulties with not using the test scores to make decisions (e.g. selecting people with low test scores who are likely to fail). If test scores are used to select applicants, then there will be a problem of range restriction (i.e. the full range of applicant test scores will not be represented in the correlation). Such range restriction, due to selection effects, will lower obtained correlations between test and criterion. By contrast, concurrent validation has the advantage of being easier to conduct since it does not involve a significant time interval between testing and criterion measurement. Also, the sample size is likely to be larger, since it involves a usually quite large current population e.g. all present employees in a job role rather than the few people appointed during selection. Such concurrent validity coefficients are sometimes used to assess the potential of the test for selection to a job role. However, this has the disadvantage that the current population is often not a good representation of the applicant pool: again, there is the problem of range restriction in the job holders scores compared to job applicant scores. Furthermore, in concurrent validation, the test may measure attributes not present in the applicants. For example, if a job involves learning necessary skills, which the test then measures, there may be a good correlation between test and job performance because it measures differences in the extent to which these skills have been learned. However, the applicants would not possess these skills, and so there could be no relationship between applicant test scores and future performance. With this selection effect, if the people involved in a concurrent validity study are quite different from the general applicant pool, then concurrent validation may not accurately assess the tests usefulness for decisions with applicants. 5.0.2.4 The need to examine the correlation scatterplot. Whether the criterion related validity coefficient is concurrent or predictive, there is a need to examine the scatterplot of the relationship between test scores and criterion scores. A number of relationships are possible beyond the classic ‘ellipse’ relationship. For example, the figure below shows a heteroscedastic relationship. Here, as X goes up so does Y, but so too does the spread of scores in Y. This heteroscedastic correlation means that, in this case, as X increase there is increasing error in the prediction of Y. There is a similar heteroscadastic relationship for the figure below. However, here there is also no relationship between X and Y for the higher scores on X. This relationship is termed a twisted pear (Fisher, 1959), and it is relatively common in psychometrics. Other relationships may be described as curvilinear. For example, in the figure below case it is only the middle scores on X that are associated with high scores on Y. If X is used for selection, then we would need two cut-off scores: one for scores that are too low and one for scores that are too high. Yet other relationships may be described as spurious. In the figure below there appears to be a reasonable correlation between X and Y when the data set is treated as a whole. However, this relationship is spurious in that it is entirely a consequence of the two different populations, which when considered separately reveal no correlation between X and Y. Other misleading relationships are also possible. For example, the figure on the left below shows a reasonable relationship between X and Y. The three types of data points shown may, for example, have come from three different local validation studies. The figure to the right shows the same data, but here the data within each set has been averaged. Correlations that are based on averages are termed Ecological correlations and they tend to overestimate the strength of a relationship. 5.0.3 Construct validity This is the extent to which the test has accumulated evidence illustrating the nature of its theoretical construct. This is an ongoing process, drawing on many interrelationships, which increasingly illuminates the extent to which the test may be said to measure a construct. As such, all forms of validity evidence mentioned above are relevant to construct validation, but it is broader than these procedures alone. A number of sources of evidence can contribute to construct validation, including: 5.0.3.1 The test’s correlation with other tests. For example, scores from a test of spatial ability should correlate with other established tests that claim to measure a similar construct of spatial ability. However, the correlations such not be too high (i.e. about 0.6 rather than 0.9), since too high a correlation means that the tests duplicate each other. Of course, correlating a test with other established tests is only of use if the established tests have been shown to relate to observable behaviour outside of the tests. Furthermore, this test should not correlate very well with other tests that claim to measure quite different constructs. For example, a test of spatial ability should not correlate highly with knowledge of word meanings or mathematical ability. 5.0.3.2 Meaningful group differences. For example, a test of spatial ability should give significant differences in the average scores between such groups as, on the one hand, architects and template designers, compared with groups that are not considered to require spatial ability. 5.0.3.3 Intervention or experimental effects. For example, interventions such as visualisation training programmes or practical block building activities may be expected to improve scores on a test of spatial ability. Such interventions may take the form of an experiment, where, for example, one group receives the intervention and a comparison group does not. The above are sources of evidence that support particular inferences made about the test scores. Validity then, really about establishing whether there is support for the inferences made about the test scores. There should be as many validity studies as there are inferences. 5.1 The relationship between reliability and validity: The maximum validity coefficient between a test and criterion is equal to the square root of the two reliabilities multiplied together: $ validity_{max} = $ A test cannot be valid if it is not reliable. However, a reliable test is not necessarily valid. Even if we have a test with little influence from error, what is being measured may not be meaningful and useful. "],["outcomes.html", "Chapter 6 Tests, Decisions and Outcomes 6.1 Test Validity, Selection Ratio, Base Rate and the Quality of Decisions. 6.2 Calculating the Probabilities of Decision Outcomes 6.3 Calculating the Cash Benefits of a Selection Strategy 6.4 Indicative Reading", " Chapter 6 Tests, Decisions and Outcomes When evaluating the usefulness of a selection strategy, one possible approach is to categorize the outcomes of this strategy, or decision process, simply in terms of a two-way split. The possible outcomes from this categorization are illustrated in figure 1 below. In the above figure, a cut-off score on test performance has been set at the average of the test performance for the applicant pool. In this case all those with above average performance on the test would be selected. Also in this figure, all those in the applicant pool who would score above average on criterion performance (before the application of the test) have been classified as satisfactory. This 2x2 split gives four possible outcomes for any decision. False Negatives (FN) and False Positives (FP) are incorrect decisions. True Positives (TP) and True Negatives (TN) are correct decisions. 6.1 Test Validity, Selection Ratio, Base Rate and the Quality of Decisions. From the previous figure, it is clear that any valid test has the potential to increase the proportion of correct decisions (i.e. TP + TN / Total) compared to a less valid test or random selection. However, the actual contribution of a test to the quality of decisions is affected not just by the validity of the test, but also by the cut-off score used and the level of criterion performance that is considered to be satisfactory. In the previous example, these were both set at the average level of performance for both test and criterion within the applicant pool. In fact, these two factors can each be set at any level. Setting a high cut-off score on the test will ensure that proportion selected from the applicant pool will be low. The proportion selected from the applicant pool is termed the Selection Ratio, and is a direct reflection of the cut-off score if the test alone is used in the decision process. In the example, the Selection Ratio is set at 0.5 (i.e. 50% of the applicant pool will be selected). Setting a high level of performance on the criterion will ensure that the proportion of applicants who would be satisfactory will be low. This proportion of potentially satisfactory applicants is termed the Base Rate, and it is a direct reflection of the level of acceptable criterion performance. In the previous figure, the Base Rate is set at 0.5 (i.e. 50% of the applicants are considered to be potential successes) 6.2 Calculating the Probabilities of Decision Outcomes When using a test or selection process of no validity, the probability of each decision outcome is a function of the selection ratio and base rate. If selection is random then the probability of making a true positive decision [P(TP)] may be calculated as Base Rate (BR) x Selection Ratio (SR). The probability of the other outcomes can then be calculated by subtraction. For example, table 1 overleaf illustrates the probabilities of each outcome for a particular selection ratio (i.e. 0.2) and base rate (i.e. 0.3) when there is no validity. When a test of some validity is used, then the probability of making a True Positive decision is increased. This may be calculated as: $$\\large P(TP) = BR \\times SR + r_{xy} \\sqrt{(BR(1-BR) \\times SR(1-SR))}$$ Again, the probability of the other outcomes can be calculated by subtraction. These probabilities can be compared with random selection and with tests of differing validities The above equation can be used to calculate the probabilities of decision outcomes from a knowledge of validity, base rate and selection ratio. An alternative approach to the evaluation of outcomes is to use the tables constructed by Taylor-Russell (1939). These tables show the proportions (or probabilities) of satisfactory employees [TP / (TP + FP)] expected through the use of a test of given validity, for a given base rate and given selection ratio. The proportion of satisfactory employees [TP / (TP + FP)] is known as the Validity Rate. The table below shows an extract from the Taylor-Russell tables illustrating the relationship between Base Rate, Validity Coefficient, Selection Ratio and Validity Rate. Previously the base rate was defined as the proportion of potentially satisfactory applicants in the applicant pool. The validity rate has just been defined as the proportion of satisfactory employees from those selected. Thus the Incremental Validity from a test is: Validity Rate - Base Rate. Using Taylor-Russell tables: What is the Validity Rate for a test of validity 0.7, with a base rate of 0.6 (i.e. 60%) and a selection ratio of 0.2 (i.e. 20%)? 6.2.1 What is the Incremental validity? An examination of the Taylor-Russell tables reveals that the proportions of satisfactory employees increase as test validity increases, particularly when the selection ratio is low and the base rate is around 50%. The impact of changing the selection ratio and base rate, when using a test with at least some validity, is illustrated in figure 2 and 3 on the next page. It is possible to calculate validity rates from an actual scattergram of the relationship between test scores and criterion scores. However, for an accurate picture, the sample size would need to be large. Figure 4 below, gives an example, showing the number of people within each outcome category for a quite valid test. Here the criterion-related validity is about 0.5. 6.3 Calculating the Cash Benefits of a Selection Strategy An organisation may be concerned with the return from increasing the accuracy of the selection process. A formula for calculating the expected criterion performance for a selected group of people is shown below: \\({\\bar{y}£}_{Av}= r_{xy} \\times (z_{Av}) \\times y£_{sd}\\) Where \\({\\bar{y}£}_{Av}\\) is the average predicted criterion performance in pounds; \\(r_{xy}\\) is the criterion-related validity for test or procedure; \\(z_{Av}\\) is the average test performance for selected group in z scores; and \\(y£_{sd}\\) = standard deviation of criterion performance (in pounds) • For a test with validity (\\(r_{xy}\\)) 0.5 the slope of the regression line will be an average 0.5 increase in the Standard Deviation of Y (\\(y£_{sd}\\)) per unit increase in the Standard Deviation of X. • For this test if those hired score on average one standard deviation above the mean on the test (i.e. \\(z_{Av}\\) = 1), their predicted criterion score will be 0.5 of a standard deviation above the mean on the criterion Thus if we can calculate the pound value of the SD of Job performance we can then calculate how much the test will benefit the organisation compared to the current average worker. Criterion performance itself may be measured in many ways, but one useful measure is the pound value of output. Research (e.g. Schmidt et al 1986) indicates that the standard deviation of the pound value of output is 40% of salary. Thus, if the average salary for a job is £25,000 per year then the standard deviation is £10,000. If we assume that job performance is normally distributed this would mean that workers who perform at one standard deviation above the mean (i.e. who lie at the 84th percentile) have an output £10,000 higher than the average worker. Thus if we select people who score on average one standard deviation above the mean on a selection test (Av.Z = 1), with validity (rxy) of 0.5, for a job with an average salary of £25,000 (i.e SDy = £10,000). Then the predicted criterion performance in pounds per year per person is £5,000 above the mean (i.e. \\(\\bar{y}/year/person/ = r_{xy} \\times z_{Av} \\times y_{sd} = 0.5 \\times 1 \\times £10,000 = £5000\\)). An Illustration of the Potential Cash Benefits from Valid Selection A company advertises nationally for about 20 management trainees per year. On average 800 applicants are given a selection test. Therefore the selection ratio is about .025. That is the top scoring 2.5% are employed. These employees will have scored about 2 standard deviations above the mean on the selection test. This selection test has a criterion-related validity of 0.65. If the average wage for management trainees in this company is £20,000 p.a., then compared to using a test of no validity this test will profit the company by how much money per person per year? In 10 years of selection for the above scenario (and assuming that each employee is a trainee for just one year) what will be the total profit from the trainees for this test? The above figure is a something of an overestimation since one would need to deduct the costs of the testing procedure. 6.4 Indicative Reading Cook, M. (2009). Personnel Selection: Adding Value through People. 5th ed. Chichester: John Wiley &amp; Sons Ltd. Schmidt, F.L. and Hunter, J.E. (1998). The Validity and Utility of Selection Methods in Personnel Psychology: Practical and Theoretical Implications of 85 Years of Research Findings. Psychological Bulletin, 2, 262-274. Sturman, M.C. (2003). Utility Analysis : A Tool for Quantifying the Value of Hospitality Human Resource Interventions. Cornell Hotel and Restaurant Administration Quarterly, 44, 106-116 "],["TBF.html", "Chapter 7 Test Bias and Fairness 7.1 The Potential for Adverse Impact 7.2 How to investigate test bias 7.3 Content Validity and DIF Analysis 7.4 The Fair Use of Tests 7.5 Sources of Information and Advice 7.6 From the website: 7.7 Indicative Academic Reading", " Chapter 7 Test Bias and Fairness It is not unusual to find differences in average test scores between groups within a population. For example, many American Psychometric textbooks cite an average difference between particular groups of 15 points on standardised IQ tests. One such textbook, Kaplan &amp; Saccuzzo (1997), state that “this is not a debatable issue”. However, why such differences exist certainly is a debatable issue. 7.1 The Potential for Adverse Impact A difference of 15 points is about one standard deviation on standardised IQ tests. Such a difference would mean that only about 16% of the lower scoring group score above the mean of the higher group (see figure 1). If such tests were used in a selection context then clearly there is the potential for adverse impact (i.e. the systematic rejection of a substantially higher proportion of one group compared to another) Many authors make a distinction between test bias and test fairness. For these authors, whether or not a test is biased is a matter of “objective statistical indices that examine the patterning of test scores for relevant subpopulations” (Gregory, 1996, p.263). Whether a test is fair is considered to be a matter of social values, social consequences and ethical principles. Tests should be shown to be unbiased, but you may still not consider them to be fair. 7.2 How to investigate test bias A central question is whether tests have differential validity. This is a question of whether inferences based on test scores are equally meaningful, useful and appropriate for different groups. As such, investigating test bias will involve drawing upon all the aspects of test validation when making comparisons between groups. One obvious place to start is with the content validity of the test. Tests should take care to ensure that, as far as possible, the specific item content is not favouring one group over another. Thus, a test of verbal reasoning should endeavour to include words that are equally familiar to all groups who may take the test. Other tests may intend to measure understanding of word meanings rather than verbal reasoning e.g. a test of English vocabulary. In this case, it is much more difficult to exclude items that favour the majority culture or age groups, etc. The important point here is that, if such tests are to be used, there should be a clear requirement for this test. 7.3 Content Validity and DIF Analysis Biased items can sometimes be identified through Differential item functioning (DIF) analysis. This method involves equating groups on the basis of overall score by taking samples from each group with about the same scores. Next, individual items are examined for any differences between the two ‘matched’ samples from the groups. Those items that show a significant difference should be removed. ## Criterion-Related Validity and Bias An important approach to the evaluation of bias is to examine differences in the relationship between test scores and external criteria. Such criterion-related studies give the most direct assessment of the extent to which the test is misrepresenting the lower scoring groups. There are a number of possible outcomes with such studies, and some of these are illustrated in figures A to D below. In figures A, B and C using a single prediction line (regression line), derived from putting both groups together, would misrepresent the relationship between test score and criterion for both groups. Here then, the uniform use of the test would lead to bias. Using separate regression lines for different groups, and taking the highest performing members of these groups may avoid this problem. However, such a top down quota system may not be legal, even though such quota sampling would reduce adverse impact. In figure D, a single regression line does fit the relationship between test score and criterion for both groups. Statistically, the test has not been shown to be biased, even though one group has a lower average test performance. However, although the test has not been shown to be biased, the situation illustrated in figure D could come about as a result of bias in both the test and the criterion against which it is being compared. One further point to consider about the situation illustrated in figure D is that even though they may be large differences in mean test scores between the two groups, the difference in predicted criterion performance will not be so large. Figure E illustrates a situation where a test with a criterion related validity of 0.5 will give rise to a predicted mean difference in criterion performance that is half the difference in mean test performance. Most US textbooks argue that there little evidence for differential validity of ability tests majority and minority Americans, but there is little research in the UK. However, on the whole, there appears to be little evidence of single group validity i.e. where the test is valid for one group but not another 7.4 The Fair Use of Tests Test manuals should be checked for what has been done to try and ensure the fairness of the test, and assess the extent to which there are any group differences in test performance. Any group differences would not necessarily mean that the test was biased, but the effect of such differences should be monitored. Group differences on the test are likely to lead to some adverse impact because, as a general rule, separate norm tables for evaluating test performance should not be used. The Commission for Racial Equality (the CRE) did not recommend using separate norm groups. The norm group should be, as far as possible, similar to the applicant pool in terms of gender and ethnic representation, age, and educational background. Whether a test, or indeed any other selection method, contravenes the Equality Act is a matter of whether the assessment (and format of assessment) is relevant to the job or role. If, however, there is adverse impact for a particular group, then it is particularly important to assess such issues as: whether the abilities and skills being measured are really relevant to the job; whether the tests are at the right level for the job and applicant pool; whether there are alternative methods with comparable validity but less adverse impact; whether the tests predict job performance for all groups equally well; whether training programmes for disadvantaged groups can offset differences. It should be noted that the ‘presumption of discrimination’ may be greater if minority groups are underrepresented in less specialised jobs. It should further be noted that adverse impact is not only a problem with standardised tests. 7.5 Sources of Information and Advice There are a number of sources that can be referred to when using tests, or indeed, any other decision tools e.g.  The Equality Act 2010 is the law which bans unfair treatment and helps achieve equal opportunities in the workplace and in wider society (Click here for link to Home Office Guidance). 7.6 From the website: 7.6.1 What’s included in the Equality Act? The act replaced previous anti-discrimination laws with a single act to make the law simpler and to remove inconsistencies. This makes the law easier for people to understand and comply with. The act also strengthened protection in some situations. The act covers nine protected characteristics, which cannot be used as a reason to treat people unfairly. Every person has one or more of the protected characteristics, so the act protects everyone against unfair treatment. The protected characteristics are: • age • disability • gender reassignment • marriage and civil partnership • pregnancy and maternity • race • religion or belief • sex • sexual orientation The Equality Act sets out the different ways in which it is unlawful to treat someone, such as direct and indirect discrimination, harassment, victimisation and failing to make a reasonable adjustment for a disabled person. The British Psychological Society (the BPS) has launched a website at www.psychtesting.org.uk. Within this site at http://www.psychtesting.org.uk/the-ptc/guidelinesandinformation.cfm You can find: • A Code of Good Practice • Draft Data Protection and Privacy Issues in Employment Related Settings • Principles for use of Published Psychological Tests in Research • Dyslexia and Occupational Testing • Visual Impairment and Psychological Testing Test Publishers also have a number of Resources and Guidelines that often address ‘Best Practice’ issues and fairness: Oxford Psychological Press (OPP) has a useful website offering HR tips and techniques at http://www.opp.eu.com/Pages/home.aspx CEB: Talent Measurement. A large international company that has now taken over ASE (the previous publishers of the GAT and MOST). Offers some interesting resources http://www.shl.com/uk/expertise/disability-guidelines/ https://www.cebglobal.com/shl/uk/expertise/disability-guidelines/faq 7.7 Indicative Academic Reading Barrett, A. (2011) Using psychometric test scores: Some warnings, explanations, and solutions for HR professionals [Online] (Accessed 12th Dec 2013) http://www.pbarrett.net/#whitepapers:1 Chung-Yan, G.A. &amp; Cronshaw, S.F. (2002). A critical re-examination and analysis of cognitive ability tests using the Thorndike model of fairness. Journal of Occupational and Organizational Psychology, 75, 489-509. Mathews, S. (1999). Testing People Fairly: The Best Practice Handbook. Windsor: NFER-NELSON. Meade, A.W. &amp; Fetze, M. (2009). Test Bias, Differential Prediction, and a Revised Approach for Determining the Suitability of a Predictor in a Selection Context. Organizational Research Methods,12, 738-761. Biddle, D.A. &amp; Noreen, P.M. (2006) Validity Generalization Vs. Title Vii: Can Employers Successfully Defend Tests Without Conducting Local Validation Studies? Labor Law Journal,57, 216-237. Rafilson, F. (1991) The case for validity generalization [Online] (Accessed 12th Dec 2013) http://ericae.net/edo/ED338699.htm "],["TBG.html", "Chapter 8 Test Feedback Guidance", " Chapter 8 Test Feedback Guidance The written feedback report has a 1,500 word limit and should address all the elements outlined in the written feedback checklist. In addition, you should append to the written report the completed Test Data Record Sheet, with all calculations, and the sheet showing plotted 68% percentile Confidence Interval (68% percentile CI) for each of the four scores. This appended information does not contribute to the overall word-count. The main body of the report should not need to make reference to the appended information, but is included within the portfolio so that markers can assess the clarity and accuracy of the underlying information and calculations. The assessment of the written feedback report includes: • The accuracy and clarity of Data Record Sheet and plotted 68% percentile CIs • The extent to which each element of the written feedback checklist has been addressed within the main report • The extent to which the main report provides a professional, informed and useful feedback Please see the complete set of marking criteria within the unit handbook. There are some challenges when attempting within the written report to convey effectively the statistical and technical information contained within the Data Record Sheet. For the feedback to be useful, information needs to be conveyed in ways that are accurate and full, but also in a non-technical way as appropriate for the target hearer/reader. Below are some examples of how to convey some technical aspects within feedback. Within these examples, the phrase ‘fairly confident’ is used to express the meaning of the technical 68% Confidence Intervals. For your oral or written feedback you do not need to address the 95% Confidence Intervals (but these do need to be shown within the Test Data Record Sheet). Please note that there may be many ways in which the following example extracts could be appropriately expressed. Please consider how these extracts may be improved. For the measure below, an adult general population comparison group is used rather the adult offenders when scoring your test score profile. Please also note that these are only extracts. On the Dark Triad - Psychopathy, the individual scored 30 out of a possible 35. The score has been compared with the performance of 80 UK Employees who have previously taken this measure. The score of 30 is higher than 85% of norm group. No tool is perfectly accurate in its measurement and we would always expect some margin of error around a score. This is the extent to which the score underestimates or overestimates the individual’s current in the area measured by this test. Taking this likely margin of error into account, we can still be fairly confident that the individual’s current score on similar psychopathy measures (taken under the same testing conditions) would lie somewhere between 27 [Raw score - SEM] and 33 [Raw score + SEM] out of 35. The lower limit of this range (a score of 27) would place the individual higher than 70% of UK adults, and the upper limit of this range (a score of 31) would place the individual higher than 95% of the UK adults. Overall, the individual score on the Psychopathy scale can be described as lying within the highest range when compared to Adult Employees. The individual was also administered the Narcissistic subscale of the Dark Triad. Because these two measures use the same comparison group (i.e. UK adult employees), the individual’s score on these two measures can be directly compared. To do this the score on each measure has to be converted to a standard scale. The middle-point of this standard scale has a value of 50 and is equal to the middle-point performance of the UK adult employees. On this scale, most people score between 40 and 60. Using this way of scoring the measure, the individual’s standard score on Psychopathy is 65 and the individual’s standard score on Narcissism is 59. Taking into account measurement error on both tests, the 6 point difference between these two standard scores is large enough for us to be fairly confident that there is a reliable difference here. That is, the individual would be likely to attain a similar difference in scores between these two scales on repeated testing. Please note that the above passage would of course be affected by the particular difference in standard score in comparison to the value of the standard score SEdiff (in the above extract, the scores differ by more than the value of the SEdiff, but the difference is not as large as 1.96*value of the SEdiff) "],["SP.html", "Chapter 9 Statistics Primer 9.1 Frequency distributions 9.2 Measures of central tendency 9.3 Measures of dispersion or variability", " Chapter 9 Statistics Primer To understand and appreciate how psychological tests should be used, it is necessary to understand a certain amount of statistics. These worksheets are intended to help you with some of the basic principles and should be completed before attending the Ability Assessment unit. Those of you who have taken statistics courses before may well find these sheets rather easy, but we hope you will appreciate that they have to be designed for those who have not met statistics before and who may find the whole idea rather daunting. The major topics covered are: 1 Frequency distributions 2 Measures of central tendency 3 Measures of dispersion 4 Z scores and percentiles 5 Standard error and Confidence limits 6 Correlation coefficients and scattergrams 9.1 Frequency distributions 9.1.1 Drawing frequency distributions A frequency distribution shows the number of times each score occurs in the set of scores under examination. Here is a set of scores: 5 7 5 8 8 5 6 7 6 7 7 4 6 10 6 8 7 9 To draw the frequency distribution for these scores, you begin by counting the number of times each score occurred. You can see that the score of 5 occurred 3 times, the score of 6 occurred 4 times. To put it in slightly more technical language, the score of 5 has a frequency of 3, the score of 6 has a frequency of 4. It is helpful to draw up a table showing each score and the frequency with which each score occurred, like this: Table 9.1: ** Score Frequency 4 1 5 3 6 4 7 5 8 3 9 1 10 1 By adding up the entries in the frequency column, you get the total frequency which must equal the number of scores you have. In this example there are 18 scores, and the total of the frequencies does equal 18. You will see that there are different values in the column headed Score. To show that the number (score) we are referring to can take on different values, we usually label it x. Frequency is usually given the label f. So the usual way a table such as this is shown is like this: x f 4 1 5 3 6 4 and so on…. When you add up a set of numbers, you obtain the total. In statistics, the total or sum is represented by the Greek letter \\(\\Sigma\\) which is named sigma. So the sum of the f column is represented as \\(\\Sigma f\\). If you add up a set of scores, these are referred to as x, so the total is \\(\\Sigma x\\). You will find that you often need to have a graph showing the frequency distribution. There are various ways to do this, and one of them is to plot points on a graph where each point represents the frequency of a score, and then join up the points. (It is usual to have frequency on the vertical axis.) The points from the table above have been plotted in the graph below- check that you understand how the location of each point is decided. If you connect the points with lines, you obtain a frequency polygon, as shown below (Fig1.2). 9.1.2 Grouped frequency distribution If you have a lot of data, and a wide range of scores, plotting the frequency of every score can be unwieldy. Suppose you have recorded the ages of a set of respondents, you might find they vary between 15 and 72. To make a frequency plot more manageable, you can plot the frequency for classes or groups of ages rather than for each age. So you might record the number of cases in each age group, using groups like these: 15-24, 25-34, 35-44, 45-54, 55-64, 65-74. This means you only have 6 classes rather than the 58 you would have if you tabulated every single age. When creating a grouped frequency distribution, you have to examine the scores to see what class intervals to use. In the example of ages, it would be useless to have just one class of 15-74, since everyone would be in it and you would have lost all the precision gained by asking people their ages. Having just 6 categories is rather few, and you might find it better to have classes of 15-19, 20-24, 25-29, 30-34 and so on. In general, somewhere about 9 or 11 classes is most useful. Note that the limits of each class are unique- you could not have classes of 15-20, 20-25, 25-30 etc because you would not know into which class to put a score of 20. 9.1.3 Skewed distributions The distributions shown in Fig 1.1 and 1.2 are more or less symmetrical. But sometimes you will find distributions like those shown in Fig 1.4, where most of the scores are at one or other end of the scale of scores. When the distribution is like the left-hand panel of Fig 1.4a with most of the scores piling up at the left-hand side, it is described as positively skewed, and when the scores are piling up at the right-hand side as in the right-hand panel it is negatively skewed. (We all find this terminology confusing and easily forgot which is which!) 9.1.4 The normal distribution curve The normal distribution curve, which is fundamental to statistical analysis, is illustrated in Fig 1.5. It is a frequency distribution, with the frequency of a score on the vertical axis, and the scores along the horizontal axis. If we take large sets of data for biological functions such as body height, we find that the resulting frequency distribution is close to a normal curve. 9.2 Measures of central tendency 9.2.1 Mode, Median, Mean We often need to have one figure which represents a set of scores. This is known as a measure of central tendency, and there are three such measures in common use. The MODE is the most frequently occurring value. For the set of scores in Exercise 1.2, the score which occurs most often is 10 (it has a frequency of 8) so this is the mode. For the scores shown in Exercise 1.1, the score which occurs most often is also 10 (with a frequency of 5). The MEDIAN is the value that divides the distribution of scores in half so that half the scores fall below the median and half fall above it. To find the median, put the scores in ascending order. If there are an odd number of scores, the median is the middle score. If there are an even number of scores, average the two middle scores. For the set of scores in Exercise 1.1, if we put them in ascending order they look like this: 6 7 8 8 8 9 9 9 9 10 10 10 10 10 11 11 12 There are 17 scores, so the median will be the one that has 8 scores below it and 8 scores above it. Here the median is 9. The MEAN is what is in normal speech referred to as the average (but when talking statistics, always refer to is as the mean). To find the mean, add up all the scores and then divide by the number of scores there are. For the numbers shown above, the sum is 6+7+8…+12 = 157. There are 17 scores, so the mean is 157/17 = 9.24 (to two significant places,which is usually the level of accuracy we work with). As noted above, the sum of a set of scores is written as x. The number of scores in a set is referred to as n, so the formula for the mean is x/n. The mean is usually shown as , so the full formula for the mean is: = \\(\\Sigma x/n\\) (This is the arithmetic mean. There are other types such as the geometric mean and the harmonic mean, but you are not likely to need them. If you just refer to the mean, the reader will assume you are referring to the arithmetic mean.) 9.2.2 When to use the mode, median or mean The mode is not commonly used in psychological statistics except for reporting the results of surveys or the standardisation of psychological tests when it can be useful to know which response or score was given most frequently. The mean is the most commonly used measure of the central value. But if you have a few scores that are very different from the others (known as outliers), then the mean can be misleading because it is strongly affected by a few aberrant scores. If the distribution of scores is clearly skewed, use the median. In some cases scores are ranks such as 1st, 2nd, 3rd and so on. When you have rank scores, the median is the appropriate measure of central value. 9.3 Measures of dispersion or variability 9.3.1 The concept of variability Central tendency is only one feature of a set of numbers. Another most important feature is the spread or variation of the scores in the set. The table below shows some data from three groups of people who took a test when subjected to noise, a background of music or in silence. Find and write in the means. You will see that the means for groups A and B are the same; but you can see that the scores in group A are less variable than the scores in group B. Group A scores vary between 9 and 25, whereas group B scores vary from 3 to 27. For reasons that will become clear later, we need to be able to express the variation within a set of scores as well as the central value (mode, median or mean) of the set. How can we do this? Table 9.2: ** Group.A.Noise X X.1 Group.B.Music X.2 X.3 Group.C.Silence X.4 X.5 S1 10 NA S9 12 NA S17 22 NA S2 15 NA S10 9 NA S18 19 NA S3 9 NA S11 3 NA S19 7 NA S4 11 NA S12 25 NA S20 17 NA S5 20 NA S13 8 NA S21 24 NA S6 25 NA S14 16 NA S22 28 NA S7 18 NA S15 20 NA S23 20 NA S8 12 NA S16 27 NA S24 23 NA NA NA NA NA NA NA Sum: NA NA Sum: NA NA Sum: NA NA NA NA NA NA NA NA Mean: NA NA Mean: NA NA Mean: NA NA 9.3.2 Range; Interquartile range The RANGE of a set of scores is simply the difference between the highest and lowest scores. So in group A the range is 25-9 = 16. What is the range for group B? Range gives an indication of the spread of the scores, but of course it depends completely on just two figures from the whole set, the highest and the lowest. One very low or very high score will produce a large increase in the range, and this might be quite misleading. So we need a measure of variation that is less influenced by a single aberrant score. One measure is the INTERQUARTILE RANGE. You will remember that the MEDIAN is that score which divides the set into two halves, with half the scores falling below the median and half the scores falling above it. The median is also the 50th PERCENTILE, which means 50% of the scores fall below it. We can also have a 25th percentile, which is the score below which 25% of the scores fall, a 75th percentile, a 90th percentile etc. (You will make use of percentiles later on.) The interquartile range is the difference between the 25th and 75th percentiles. You may come across the semi-interquartile range, which is the interquartile range divided by 2. The interquartile range, like the range, only uses two figures from the set to express the variability in the set, and so it ignores most of the numbers. A better measure of variation would be one that used all the numbers in the set, not just two of them. 9.3.3 Variance and standard deviation The problem of finding a number to express how much variation there is in a set of scores is tackled by looking at the mean of the set, and taking the difference between each score and the mean. In the table below, I have listed the differences between the mean and each score for group A; you should fill in the differences between the mean and each score for group B. You will find that the total of the (x- ) column is zero: so this figure is not going to be very helpful as an indication of the variation in the set of scores! The way round this is to square each of the numbers in the (x- ) column, which gets rid of all the negative numbers, and then add them up. Again, I have done this for group A, and you should fill in the table below for group B. The difference between each score and the mean of the set of scores (x- ) is the deviation for that score. In order to get an idea of the variation in the set, it is sensible to take the average of the squared deviations. As you know, the average is the sum/n. (Note: Later we shall divide the sum of the squared deviations by n-1 rather than n. This is confusing! Essentially, you need to distinguish between two situations. In the present examples, we want to know what is the mean square deviation of the data we have. But if we are using the data as an estimate of a wider population, then we divide by n-1 to obtain a better estimate of the variance of the wider population. You will find we return to this…) The sum of the deviations squared divided by n is known as the VARIANCE of the set of scores. The variance for group A is 220.00/8 = 27.50. What is the variance for group B? … So to calculate the variance, we took the deviations of each score from the mean, squared them, added them up and divided by n. To get back to our original scale of numbers, it would seem fair that after squaring the deviations we should now ‘unsquare’ them by taking the square root. So we can take the square root of the variance: this is the STANDARD DEVIATION, and is the number usually used to express the variation in a set of scores. There are various formulae for the standard deviation; they all give the same result, but some of them are easier to calculate than others. Here are three: \\(s = \\Sigma ( [S(x - )2] / [n-1])\\) \\(s = \\Sigma ( [Sx2 - (Sx)2/n] / [n-1])\\) \\(s = \\Sigma ( [nSx2 - (Sx)2] / [n(n-1)])\\) You will see that in (b) and (c) you do not have to calculate the deviations at all, but use the values of Sx2 and (Sx)2 which is the sum of x squared. It is vital that you understand the difference between these two formula, and can work them out correctly: Sx2 means that you take each score (x) and square it and then add up all the squared x-values. (Sx)2 means that you add up all the scores to obtain Sx and then square that sum. Here is a simple example: For the scores 3 4 5 6 Sx2 is 32 + 42 + 52 + 62 = 9 + 16 + 25 + 36 = 86. (Sx)2 is (3 + 4 + 5 +6)2 = 182 = 324. As you can see the results are very different, so ensure you calculate each formula correctly. To show you how to calculate a standard deviation using formula (b) s = ÷ ( [Sx2 - (Sx)2/n] / [n-1]) here are the details for finding the standard deviation for the scores in group B. Table 9.3: ** Group.B.Music X x X.1 x2 S9 12 144 S10 9 81 S11 3 9 S12 25 625 S13 8 64 S14 16 256 S15 20 400 S16 27 729 x = 120 x2 = 2308 n = 8 Sx = 120 So (Sx)2 = 1202 = 14400 And (Sx)2/n = 1202/8 = 14400/8 = 1800.00 s = ÷ ( [Sx2 - (Sx)2/n] / [n-1]) = ÷ ( [2308 – 1800] / [7]) = ÷ ( [508] / [7]) = ÷ ( [72.57]) = 8.52 Note that a standard deviation cannot be negative. If you calculate it and obtain a negative number, you have made an error. (You may have confused Sx2 and (Sx)2.) "],["SPZPER.html", "Chapter 10 z scores and percentiles 10.1 Standard error and confidence limits 10.2 Correlation coefficients and scattergrams", " Chapter 10 z scores and percentiles 10.0.1 Standard deviation of the normal curve: z scores There are three major aspects of the normal curve shown in Fig 1.5 that you should appreciate. First, it is symmetrical, with the ‘middle’ being equal to the mean. Secondly, normal curves vary in their ‘spread’. The spread of the distribution is related to the standard deviation of the scores. Thirdly, one can measure off the horizontal axis in standard deviations. For example, assume we have measured the breadth of hand of 500 British men aged 19-45 years. The measurements form a normal distribution, with a mean of 85 mm and a standard deviation of 5 (Pheasant, 1986). The frequency distribution is shown in Fig 4.1a, where the horizontal indicates hand breadth in mm and the vertical is frequency. We can plot the measurements along the horizontal in terms of the number of standard deviations they are away from the mean; a score of 90 mm is 1 standard deviation above the mean, so the 90 mm point can be expressed as +1 sd. Similarly, a score of 75 mm is 2 standard deviations below the mean, so can be expressed as -2 sd. Fig 4.1b shows the horizontal axis scaled in sd units. There are a number of points to note. First, very nearly all the distribution lies between -3 sd (70 mm) and +3 sd (100 mm) from the mean. For a perfect normal distribution curve, the precise location of standard deviations along the horizontal is known. A vital property of the normal distribution curve is that it is known what proportion of the curve lies between any given positions along the horizontal axis, when the horizontal is in standard deviation units. One way to appreciate this is to remember that the normal curve is symmetrical. This means that 50% of the curve is above the mean, and 50% is below. (If we take the area under the curve as 1.00, we can say that 0.50 of the distribution is above the mean and 0.50 is below it.) The mean corresponds to a value of 0 when the horizontal is in standard deviation units, because it is 0 standard deviations away from the mean. So we know that 0.50 of the curve is below the position where the horizontal score in sd units is 0. Statisticians also know how much of the curve lies below +1 sd, -2 sd, +1.96 sd and any other value we care to suggest. For example, it is known that 0.8413 of the area under the curve falls below +1 sd. So how much is included in the area between the mean (0 sd) and +1 sd (i.e. 1 sd above the mean)? You can see from Fig 4.2 that the answer is 0.8413 - 0.500 = 0.3413. In percentages: 84.13% - 50% = 34.13%. When we express a score in standard deviation units from the mean, they are known as z-units. So +1 sd = +1z, -2.46 sd = -2.46z. 10.0.2 Using normal curve (z) tables To find the area under the curve for any z value, use the table of the normal curve. If you look at the z table, you will see there are entries for values of z from 0 (i.e. where z equals the mean) to z = 3.9. The entries are the same whether z is + or -, so only one set of values is given.) An example of one line in that table is: The z table lets us answer various questions about how one score compares with others. Remember that the z table uses sd units, so if we are dealing with ordinary scores, we have to transform them into z units before we can use the table. Here is an example. The mean for hand breadth is 85 mm and the standard deviation is 5 mm. So what proportion of the men measured had a hand breadth less than 95 mm? To answer this question, transform the score of 95 mm into z units by finding the difference between the score (95) and the mean (85) and dividing the result by the sd (5). In this case, you get a value of +2. So 95 mm is +2z (2 standard deviations above the mean). In the table of z, you will find that the area in the larger portion of the curve is .9772; therefore the proportion of men who had a hand breadth less than 95 mm is 97.72%. 10.0.3 Comparing scores using z values Using z scores, we can compare the scores of tests where the mean and standard deviation differ. If Joseph Smith has a hand breadth of 90 mm, we know he has a large hand, because his hand is larger than 84.13% of the population. (90 mm is 5 mm or 1 standard deviation above the mean, and the z table tells us that the proportion of the population falling below this value is .8413 or 84.13%.) Suppose Katherine Smith has a hand breadth of 81 mm. Is this a large hand? When you know that the mean hand breadth for British women is 75 mm and the standard deviation = 4 mm, you can find out how many of the population of British women have a smaller hand than Katherine Smith. Her hand is 6 mm wider than the average; as the standard deviation for women’s hands is 4 mm, Katherine’s is 1.5 standard deviations larger than the mean. From the z table you will find that this means Katherine’s hand is larger than .9332 or 93.32% of British women’s hands. So Katherine has a larger hand, compared with other women, than Joseph, compared with other men. You would never have been able to draw this conclusion if you had only known that Joseph’s hand was 90 mm and Katherine’s was 81 mm! The z tables are particularly useful when you are dealing with the results of standardized tests. Once you know the mean and sd of the normative sample who were used to standardize the test, you can use z tables to discover whether any particular score is ‘high’ or ‘low’ by finding out the proportion of the normative sample who scored less than the score of the person you have tested. 10.0.4 z-scores and percentiles When a score is at the 84th percentile, 84% of people score below it. So you might expect that percentiles can be related to z scores, and indeed this is the case. The z table shows that when z = +1.00, the portion of the curve below z is .8413 (84.13%) i.e. if someone scores at +1z, he has surpassed 84% of the population. So the percentile score is 84. Percentile scores are valuable because they tell you immediately how a particular score stand relative to the scores of the standardization sample, the people who provided the distribution of scores on the test. However, percentiles only show the relative standing of a score compared with the standardization sample. They do not tell you the amount of difference between the raw scores of two people. 10.0.5 Calculating percentiles from a frequency distribution When you have a set of scores such as those shown in Exercise 1.2, the percentiles for any score can be found if you calculate the cumulative frequencies. To do this, put the frequencies in ascending order. Then simply add the frequencies for each frequency to the total of frequencies for the lower frequency values. As an example, the frequencies for the scores from Exercise 1.2 are shown in Table 4.1 and the cumulative frequencies are also shown. To find the percentile value for any value of x, the formula is: Percentile = [ (cf - (f/2) ) /N ] * 100 So to find the percentile value for the x value of 9 in Table 4.1, Percentile = [ (25 - (7/2) / 55 ] * 100 = [ ( 25 - 3.5) / 55 ] * 100 = [ 21.5 / 55 ] * 100 = [ 0.39 ] * 100 = 39 Table 10.1: Table 4.1 Showing how to obtain cumulative frequencies x f cf 1 1 1 2 1 2 3 1 3 4 2 5 5 2 7 6 3 10 7 4 14 8 4 18 9 7 25 10 8 33 11 6 39 12 5 44 13 1 45 14 4 49 15 3 52 16 2 54 17 1 55 10.0.6 T scores, stens and stanines A disadvantage of standard z scores is that they can be negative and use decimal places. So simpler scales are sometimes used. T scores have a mean of 50 and a standard deviation of 10. Sten scores have a mean of 5.5 and a standard deviation of 2. Stanines have a mean of 5 and a standard deviation of approximately 2. Stanine is a contraction of the phrase ‘standard nine’, and reminds you that with this scale scores run from 1 to 9. The normal curve percentages are related to stanines as shown in Table 4.2. This indicates that the lowest 4% of the scores have a stanine of 1, the next 7% have a stanine of 2 and so on. Table 4.2 Relationship of stanines to normal curve percentages Stanine Normal curve percentage 1 4 2 7 3 12 4 17 5 20 6 17 7 12 8 7 9 4 10.1 Standard error and confidence limits 10.1.1 The concept of standard error Inferential statistics involve estimating the characteristics of a population from the data obtained from a sample of that population. For example, one uses the mean of the sample to estimate the population mean. If one took a large set of samples from the population, the means of the samples would form a normal distribution. The standard deviation of that distribution is given by taking the standard deviation of the sample and dividing it by the square root of n, the number in the sample. This is the Standard Error. The Standard Error allows one to state the probability that the true mean of the population is within specified limits. From the properties of the normal distribution, it can be deduced that upon repeated testing with the same configuruation, the true mean of the population would be captured between plus or minus approximately 2 standard errors of the sample mean (which would differ every time) 95% of the time. Suppose you have taken a sample of 100 subjects from a population and found that the mean of the sample is 50, and the standard deviation is 15. The standard error is 1.5 (15/square root of 100). One can conclude that the true mean of the population would be captured between the sample mean ± 3 95% of the time if you were to repeatedly collect 100 subjects. 10.1.2 Standard error of measurement The notion of standard error is important because we can look on a person’s test score as an estimate of their ‘true’ score. We can estimate where the true score lies within confidence limits which are determined from the obtained score. This version of the standard error is known as the standard error of measurement. The standard error measurement is computed from the reliability coefficient of a test by the formula: SE(meas) = sd √ (1- r) Where sd is the standard deviation of the test scores and r is the reliability coefficient of the test. So for an intelligence test with sd of 15 and a reliability coefficient of 0.90, the SE(meas) = 15 √ (1-0.9 ) = 15 √ (0.1) = 15 (.33) = 5. Assume that we have given someone this intelligence test and they scored 110. If the person was retested, the obtained score would fluctuate. So we should take the obtained score of 110 as an estimate of the person’s ‘true’ score, the mean of the distribution of scores we would obtain if we took many readings of the person’s test performance. This hypothetical distribution of scores obtained if we retested the person many times would make a normal distribution with a standard deviation equal to the standard error of measurement. We can apply the principles of the normal distribution as we did before. According to the normal curve table, 68% of cases fall between -1sd and + 1sd of the mean; so we can argue that there is a 68% probability that the true score lies between -1 sd and +1sd of the obtained score. With the SE(meas) being 5, this means that there is a 68% probability that the person’s true intelligence test score lies between 105 and 115. Since the normal curve table shows that 99% of the distribution lies between the limits of +or- 2.58 standard deviations, we can be 99% sure that the person’s true score lies within +or- 2.58 SE(meas) values of the obtained score. As 2.58 * 5 is 13, we can be 99% sure that the true score lies within the limits of 97 - 123. An important aspect of this issue is that a score obtained by giving a person a test is only an estimate of the true score of the person, and so obtained scores should not be seen as fixed, definite features like a person’s height or weight. Rather they should be taken as a guide to the person’s real performance, a guide we can use to make estimates of the limits within which the true score lies. 10.2 Correlation coefficients and scattergrams 10.2.1 The concept of correlation A correlation expresses the extent to which two variables vary together. A positive correlation means that as one variable increases so does the other. For example, there is a strong positive correlation between size of foot and height, and a weak positive correlation between how much one is paid and one’s job satisfaction. A negative correlation is when one variable increases as the other decreases; for example, there is a negative correlation between job satisfaction and absenteeism: the more satisfied people are with their job, the lower the amount of absenteeism they show. Correlations vary between -1.00 and +1.00; a correlation of 0.00 means there is no relationship between the two variables. For example, one would expect the correlation between size of foot and job satisfaction to be about 0.00 (although I have never seen any data on this relationship!) There is one vital factor about correlations that you must not forget, summarised in the aphorism “Correlation does not equal causation”: if variables A and B are correlated, one cannot say that A causes B. It could be that B causes A, or they may both be related to some other factor that produces the variation in A and B. Some examples: absenteeism and job satisfaction are negatively correlated, but one cannot conclude that low job satisfaction causes absenteeism; it is possible that being absent a lot causes the feelings of low job satisfaction. The positive correlation between foot size and height does not mean that having a large foot makes you grow; foot size and overall height are both caused by a common genetic factor. However, correlations are used to predict one variable from another. Knowing someone’s foot size, one can predict how tall they are better than one could if you did not know their foot size. Also bear in mind that we are only dealing here with linear correlations. One can have a curvilinear relation between variables. For example, there is a well-known relation between performance and strength of motivation (known as the Yerkes-Dodson law). This states that with low motivation there is poor performance, but as motivation increases so does performance; but with very high motivation, performance declines again. So the relationship is not a linear one. But at this stage we are not concerned with anything more elaborate than a simple linear relationship. 10.2.2 Scattergrams Whenever you are looking at the relationship between two sets of scores, you should begin by plotting the relationship between them using a scattergram. Suppose we have these sets of scores: Table 10.2: ** Respondent score1 score2 1 3 2 2 3 5 3 4 6 4 5 5 5 6 5 6 6 8 7 3 3 8 4 3 9 7 9 10 8 10 11 7 7 12 4 5 13 8 9 14 5 7 15 7 8 16 6 6 17 5 8 18 6 7 19 5 6 20 8 8 To plot the scattergram, lay out the two axes of the graph so that one represents score1 and the other represents score2, as shown in Fig 6.1. To position each point, find the position for each respondent where the scores for score1 and score2 coincide. The first respondent has 3 on score1 and 2 on score2, so that determines where their data is placed. The other respondents are plotted in a similar way. Figure 6.1 shows the scattergram for the whole set of 20 respondents. 10.2.3 Pearson product moment correlation This is the measure of correlation you are likely to come across most frequently. It measures the relationship between two variables when the scores are NOT ranks. Assuming you have two scores (x and y) for each respondent, the formula is: r = N∑ xy - (∑x)(∑y) / √ [ N ∑x2 - (∑ x)2][ N ∑ y2 - (∑ y)2] To show how this is calculated, here is a small set of data: Table 10.3: ** Respondent Score Score.1 NA x y 1 3 5 2 2 6 3 4 2 4 4 4 5 5 7 From the formula you will see that we need Σxy, Σx, Σy, Σx2, Σy2, (Σx)2 , (Σy)2. The only unfamiliar expression here is Σxy, which is obtained by multiplying each value of x with its corresponding y and summing the values. The table below shows how the various expressions are obtained. Check that you can follow how the value for the correlation is reached. Table 10.4: ** X Score X.1 X.2 Score.1 X.3 X.4 X.5 Respondent x x2 NA y y2 NA xy 1 3 9 NA 5 25 NA 15 2 2 4 NA 6 36 NA 12 3 4 16 NA 2 4 NA 8 4 4 16 NA 4 16 NA 16 5 5 25 NA 7 49 NA 35 Sums: 18 70 NA 24 130 NA 86 r = N∑ xy - (∑x)(∑y) / √ [ N ∑x2 - (∑ x)2][ N ∑ y2 - (∑ y)2] = 5(86) - (18) (24) / √ [ (5) (70) - 324] [ (5) (130) - 576] = 430 - 432 / √ [ 350 - 324 ] [ 650 - 576 ] = -2 / √ [26 ] [ 74] = -2 / √ [1924] = -2 / 43.86 = - 0.046 10.2.4 Spearman Rank correlation (rho) This is a non-parametric correlation and can be used when data is ranks. To calculate rho, you rank the data for each measure (i.e. each column in Exercise 6.2), and then calculate the difference between the ranks. Square these differences, and calculate the sum of the squared differences. Then rho = 1 - [6Σd2 / N(N2 - 1) ] 10.2.5 Effect of range on correlation The correlation between two variables is dependent on a number of factors, notably the range of scores one takes. For example, consider the correlation exhibited in the Fig 6.1. If you draw a vertical line at the a value of 4.5 on the horizontal (x) axis and calculate the correlation between score1 and score2 just for the scores greater than 4.5 on score1, the correlation is smaller (I calculate it to be 0.68). 10.2.6 Regression When two variables are correlated, one can predict the level of an individual on variable x from their standing on variable y (or vice versa: one can predict height from foot size or one can predict foot size from height). When you have a scatterplot, it is possible to draw in the straight line that represents the relationship between x and y. Rather than doing this by guesswork and ‘eye-ball’ inspection, there is a procedure for calculating the best-fitting straight line. We do not need to go into the details, but it is worth knowing that one fits a straight line by the least squares method, which yields a line that minimises the distance beween the points of the scatterplot and the line. The method tells you the slope of the line (how steeply it rises) and the intercept (the point at which it crosses the axis of the graph). When you draw in the best-fitting line, it is known as the regression line. A regression line can be expressed as an equation like this : \\(x = c + by\\) where c is the intercept and b the slope. One complication is that there is one regression line when you are predicting x from y and a separate one when you are predicting y from x. The best fitting straight line when predicting y (on the vertical axis) from x is the one that minimises the vertical distance between the points on the scatterplot and the line. When you are predicting x from y, the best fitting line minimises the horizontal distance between the points and the regression line. When we use scores on a to predict scores on b, we use the correlation between a and b. In fact, the correlation coefficient squared (r2) indicates how much of the variance in b is explained by a. So if a correlates with b 0.6, then .36 (36%) of the variance in b is explained by the variance in a. 10.2.7 Multiple regression This refers to using more than one variable to predict x. Job satisfaction is correlated with pay and with level of occupation. So one can predict job satisfaction from pay and one can predict it from job satisfaction; but one can get a better prediction if one uses both pay and job level as predictors. So one would have an equation of the from: job satisfaction = pay multiplied by a + level of job multiplied by b This is an example of a multiple regression equation. "],["RTS.html", "Chapter 11 Road to (Assignment) Submission 11.1 General 11.2 Short Glossary 11.3 Part 1 - The Technical Manual 11.4 Part 2 - Sample Feedback 11.5 Part 3 - Critical Reflection 11.6 Appendix", " Chapter 11 Road to (Assignment) Submission The purpose of this chapter is to provide guidance and a checklist of tasks that are recommended for planning and writing the Psychological Measurement Portfolio. Please note that it is not exhaustive, and just represents the minimum needed. The final grade will be based on how you have engaged with the materials, the breadth and depth of your understanding, and the professionalism of the final submission. For more information, please see both the Learning Objectives for the module, and the University’s Graduate Outcomes. 11.1 General The following points apply to the whole portfolio: - Follow APA-7th edition guidelines by default. If something in this document conflicts with APA-7th formatting, then this document will take priority. (Link to APA-7th Guide) Please do not use serif fonts such as Times New Roman. Use sans serif fonts such as 11-point Calibri, 11-point Arial, or 10-point Lucida Sans Unicode. If you wish, you may use the OpenDyslexic font (Link to OpenDyslexic font) References and tables should be in APA-7th format in line with your other assignments. Avoid using Double Spacing, please use 1.5 spacing unless you have a good reason not to.1 Section numbering is optional. Indent the first line of every paragraph The portfolio must be written in British or American Standard Written English (Link to page on standard English). The portfolio should be proof-read for spelling and grammar errors (you may use Co-Pilot to do this - but include the original in the appendix). Use level 1 headings as a minimum - you may use sub-headings if you wish, but do not exceed three levels unless absolutely necessary (it probably wont be). See the Misc chapter (Link to What do you mean by sub-heading levels?) 11.2 Short Glossary Construct: Psychological attribute that is being explored. Percentile: The percentage of people who scored below a particular score (Link to Chapter on standard scores). Raw Score: The score that the Test Taker gets on the test. Might be number of correct answers (e.g. 27 out of 30), or average response to a set of personality questions (e.g. 3.5 out of 5) (Link to Misc section on Raw scores). Standard Score: A score that presents the Test Takers Raw score in the context of a norm group. Test: The method of quantifying a construct. Can also be called a measure, tool, etc. Test User: The person who will administer the test to the Test Taker Test Taker: The person who will sit down and provide answers to the questionnaires. 11.3 Part 1 - The Technical Manual Starting Points In part 1 you are responding to one of the two case studies provided on Moodle (Link to Scenarios) and providing a solution to the problem that has been provided. The first step is to carefully read the chosen case study and identify key words that imply the involvement of psychological constructs. For a guide to doing this see Week 4’s workshop materials (Link to Week 4 Workshop video). Once you have identified the constructs that are of interest in the case study, you need to choose at least 2 and match these to an appropriate measure from the list provided (Link to list of measures). What do you Mean by Boundary Condition? In Week 4 I mentioned Boundary conditions and this has caused a little confusion. There may be a particular policy that you can find indicating that people with a particular attribute (e.g. General Learning Disability) are eligible for a different type of treatment (e.g. a targeted programme tailored for this group of people). If this is the case, and scoring above or below a score on a gold standard psychometric test is part of the diagnostic criteria, then you can discuss this as a boundary or cut-off score when discussing the appropriateness of the construct (not the test). However, you must be clear that the screening test is not intended for diagnostic purposes. For a more in-depth discussion see the following link (Link to technical discussion of boundaries). Introduction Section After choosing the measures that you will use to assess the constructs of interest, you will write an introduction section that presents the constructs and measures you have chosen. This will finding and summarising evidence to support the existence of the construct (see Week 2 playlist and materials on construct validation (Link to Week 2 Playlist), evidence of the reliability and validity of the measure that you have chosen (see Week 4 materials (Link to Week 4 Playlist). In addition, you should try to find literature that has used the construct (and ideally the measure) in a forensic setting. To do this you could find the starter paper in Google Scholar, click on ‘cited by’, and search within the citing articles for key words such as ‘forensic’. Finally, you might discuss the unique information that each measure will contribute towards the profile. This will form the introduction section of the manual. Method Section You will then write a method section that provides important information about the measures that you have chosen and the norm group that the test taker will be compared with. Norm Group Characteristics (i.e. Participants section) This will include the characteristics of the participants for each measure that makes up the norm group. Demographic information can be taken from the Participant Information file, but more detailed information will be available in the stage two manuscript by Wilson and Bishop Link to starting-point papers folder). If you have never heard of a Registered Report before, see the following link: (Link to Registered Report Misc section) You should also provide the approximate date that the data was collected - even if this is just the year. Many methodological information about the data collection should also be included - again, this will come from the Wilson and Bishop stage 2 paper. Materials and Administration Instructions (i.e. Materials and Procedure) This should contain everything that the test-user needs in order to successfully administer and score the test. It will include the number of items in each scale, the number of scale points (e.g. how many different likert statements there are), an indication of which items need to be reversed (if any), and the scoring method for the scale (e.g. should they add up all of the responses, or take the mean average). You can get this information from the starter papers on Moodle (Link to starting-point papers folder). You might also consider how the test user might administer the battery. Have you been instructed to use a particular medium? Is the test timed? Do you need exam-like conditions? It’s unlikely that information like this will be available, so you can include your own instructions. Psychometric Properties (i.e. Results Section) You will then write a section detailing the Psychometric Properties of the scale. This will only be based on the norm group - you should not discuss the test-taker in part 1. You will use Jamovi to produce tables for reliabilities, means, standard deviations, percentiles (Link to Jamovi Playlist). You may also include exploratory or confirmatory factor analysis if you wish, but only do this if you can show how it is relevant to the document. After you have this information, refer to the data record sheet and the generic scoring template to get the Classical Test Theory parameters (this was covered in Week 5 – (Link to Week 5 playlist)): Raw score standard error of measurement, T score standard error of measurement, T score standard error of difference. For a short summary of these issues refer to Chapter 4 (Link to chapter on reliability). In the portfolio template there is a suggested ‘validity’ heading in the Psychometric Properties. This is optional, but could be useful. For instance, if you are arguing that Intolerance of Uncertainty is a useful screening measure because it reflects anxiety, but only in specific situations, you could provide correlations between IUS and GAD to demonstrate that the two constructs are related, but that IUS also provides unique information about an individual that is not captured by GAD-7. If a correlation is high, but not too high, (e.g. 0.4-0.6) this might indicate that there is convergent validity between the two measures, but they are also contributing something unique to the profile. If the correlation is very high (e.g. above 0.9) then you are measuring very similar constructs, and you might as well just use one of the measures. 11.3.1 Checklist for Part 1 Note that the following is a guide and simply represents the minimum to include. You will be marked on how you present this information, and how you justify your decisions, and show that you have understood the course content and met learning outcomes. This will be down to you Section Task Done Prelims Choose Scenario Identify keywords Select construct Selected measures Reviewed literature Introduction General introduction Introduce each construct Discuss construct validity of each Introduce each measure Present reliability and validity evidence Discuss the profile Method Described norm group Described materials and administration Results Calculated norm group characteristics Checked reliability of the measures Calculated Classical Test theory parameters 11.4 Part 2 - Sample Feedback In Part 2 you are presenting sample feedback for a person who has completed your measures. This is not a specific person from the institution, rather it is a template that someone else could base their own report on. Starting Points The first step will be to use the norm group data from Jamovi that you prepared in part 1 to fill out the remaining rows in the data record sheet. The first row will be the Test Takers score on the test. You have been given four z scores – these need to be converted to the scale that your measures are on. Select one z score for each of the measures that you have chosen. You can convert the z score to a Raw score using the following equation: \\[ RawScore = (NormSD \\times zScore) + NormMean \\] So for a z score of 1.8, a Raw norm group mean of 2.5, and a Raw norm group standard deviation of 1.2 you would get the following: \\[ RawScore = (1.8 \\times 1.2) + 2.5 \\] These are the scores that you will be writing up and interpreting for part 2. Now you can use the rest of the information from the results table to complete the remaining: The percentile equivalent of this score (use the percentile table to get this information) The confidence interval around this score (use the Standard Error of Measurement to calculate this). The confidence interval around their percentile score (use the percentile table to get this information). Their T-Score The 68% Confidence interval around their T score Plot the 68% Confidence interval to get the range that the test-taker is in (see Portfolio template) 11.4.0.1 Should I mention Boundary Conditions? Remember that the purpose of this screening tool is to provide one source of information that will aid the organisations decision to either: -A let the person enroll on the group rehabilitation programme immediately -B refer them for further assessment. The tool is not for diagnostic purposes, and you, as the psychometrics consultant, are not the one making the decision. So in terms of the feedback report, you will not be using diagnostic criteria as a boundary value (e.g. a score of x is indicative of…). Rather, any statements will be based on the range that their score is in relative to the norm group, and presented in terms of a typical profile (people who are in the high to highest range for this measure are more likely to…). The Report The report should include a brief introduction to yourself, the purpose of the test, the measures that were used and why, each score along with the margin of error surrounding it (68% confidence interval) and a comparison with the norm group (e.g. T scores or percentiles). Some examples of the type of language used to give feedback is given in Chapter 8 (Link to Test Feedback Guidance Chapter). It is recommended that you append a complete set of calculations (e.g. Test Data Record Sheet). A checklist for the things to include in Part 2 can be found in the Portfolio template document but is repeated below too. Again, note that this is the minimum to be included. 11.4.0.2 Checklist for Part 2 Item 1 Do you introduce yourself and/or the organisation? 2 Do you remind the reader of the purposes of the report? 3 Do you introduce each measure and briefly describe how and when each was administered? 4 Do you give a brief description of the construct each tool measures before describing the score for each test? 5 Do you give a rationale and justification for the use of each measure before describing the score for each test? 6 Do you explain clearly the nature of norm group comparison and their relevant characteristics? 7 Do you describe the meaning of the scale (e.g. percentiles) or scales (e.g. percentiles and T scores) accurately and in terms which the test taker could understand? 8 Do you communicate clearly and accurately the score for each measure? 9 Do you communicate clearly and accurately the confidence limits associated with each score? 10 Are any statements implications (e.g. risk,) supported by background information for the test (e.g. validity)? 11 Do you communicate clearly and accurately any score comparisons made across the measures taken? 12 Do you give clear guidance as to the appropriate weight to be put on the findings (e.g. such tests are only one source of information)? 13 Have you briefly mentioned any ethical guidelines associated with the report? 14 Do you give clear closure to the feedback report? 15 Have you appended the record sheet and interval plots 11.5 Part 3 - Critical Reflection In the final part of the portfolio you will discuss some of the limitations to your scale which could be considered when rolling out a larger screening policy. This can either be in report format (formal, passive, third-person), or in the form of a letter (formal, second person perspective). This will include any possible sources of bias in the constructs, scales, or norm group, that may exist as well as suggestions for alternatives or aspects that might be changed (Link to Chapter on test bias and fairness). You might also wish to include a technical critique of the methods that were used to show that you have understood their limitations and considered alternatives - e.g. issues with the test paradigm that you have used (e.g. Classical Test Theory) and suggestions for other methods (Link to Item Response Theory Video), (Link to very brief description of Generalisability Theory) This is where you will include any major limitations to the measures that you may have encountered during your literature reviewing. If you feel that any of the measures are adequate, you could also suggest follow-up tests that might be used after screening. The whole section must be evidence-based. 11.6 Appendix Checklist for Appendix The Jamovi output and Data Record Sheet calculations are requested so that I can see what happened in the event that something goes wrong. The plot of the 68% confidence intervals and Pre-Co-Pilot draft are required. Item Attached Jamovi Output Data record sheet with calculations Plot of 68% Confidence intervals Pre-Co-Pilot Draft My scrolling finger gets sore when double is used↩︎ "],["SCN.html", "Chapter 12 Scenarios 12.1 Psychometric consultancy at Birley House 12.2 Psychometric consultancy at HMP Birley", " Chapter 12 Scenarios 12.1 Psychometric consultancy at Birley House 12.1.1 The Situation You are acting as a psychometrics consultant who has been asked to develop a free-to-use screening tool to help identify the suitability of individuals for enrollment on a group treatment programme at a forensic service in the community called Birley House. The manager of Birley House wants to make the assignment of resources more efficient by identifying individuals who would immediately benefit from group treatment, and those for whom additional assessment or specialist support is required. The manager of Birley House is concerned with the service users’ attendance and engagement in the group treatment programme and has also noticed increased levels of staff sickness and turnover. The manager has asked you, as a consultant, to produce a tool that: May be administered and reported by Psychology graduates but provide sufficient information to screen for referral to a qualified test user such as a Clinical or Forensic Psychologist. Can be administered on paper or on a device such as a computer or tablet. Is free to use but is consistent with gold-standard measures. Is self-contained so that no additional information is required for it to be used, interpreted, and reported. Provides a report of the limitations of the tool to inform future development of the scheme. To assist in your task, you have been provided with a letter from the manager, Nicole Burns, providing more detail about Birley House and the programme which you should carefully consider in writing your report 12.1.2 Manager’s Letter Dear Psychologist, RE: Screening assessment pilot The report that your colleague provided on the group treatment programme was very helpful and we have started implementing some of the suggestions. As you may already know we are a service that provides offence-focused group work for females. These females are either on licence, or are serving a community sentence. The women currently travel from the community to Birley House to receive services. As we informed your colleague, the programme is delivered to six females three times a week over two months, and so non-attendance is both costly and disruptive. Now that we have a better idea of why some women do not attend, our main concern is with ensuring that this kind of treatment is provided to those who would currently benefit most. However, we have limited time and financial resources. Having discussed with the forensic psychologist, we have decided that we should pilot screening prior to enrolment to the group treatment programme. As such, I would like to commission you to produce a short, free-to-use screening tool that may be administered by our junior staff rather than their senior colleagues. The tool should identify females who are likely to have fewer issues with engaging with the programme, and those that need additional assessment and referral to more senior staff. Because we cannot be sure that all women who approach Birley House will have access to digital devices the tool needs to be both printable and deliverable on digital devices. Because the tool may be used off site, the scoring information and interpretation details should be separable from the items if needed. Although many of our staff are psychology graduates, it is possible that some of them have produce psychometric reports, so it is essential that you provide a sample report for them to follow. Finally, I would like you to prepare a separate document that highlights the limitations of the tool and some suggestions for improvements should the pilot be successful and rolled out to the wider prison population. The females we will be screening have all been imprisoned at some point in HMP Fosby, a female prison establishment. Their offences are mainly acquisitive in nature (such as theft, robbery or shoplifting), although some of the women have committed violent offences as well. The background of our service users is quite mixed, but they typically present with a history of trauma, such as child sexual abuse, and are (or were) in volatile relationships with partners. Some of the women a diagnosis of mild personality disorder with a borderline pattern qualifier, and some show intellectual or learning impairment You colleague informs me that they have already identified a possible set of tests. Please can you ensure that there are no more than four measures in the final version; that it does not exceed 3000 words; and that it is delivered no later than 25th May 2025 (unless there are mitigating factors). We are keen to work with you to help plan a way forward so that we can improve our services. Kind regards, Ms Nicole Burns Manager Birley House 12.2 Psychometric consultancy at HMP Birley 12.2.1 The Situation You are acting as a psychometrics consultant who has been asked to develop a free-to-use screening tool to help identify the suitability of individuals for enrolment on a group rehabilitation programme at HMP Birley, a category C male prison. The governor at HMP Birley wants to make the assignment of resources more efficient by identifying individuals who are likely to immediately benefit from group rehabilitation, and those for whom additional assessment or specialist support is required. The governor has asked you, as a consultant, to produce a tool that: May be administered and reported by Psychology graduates but provide sufficient information to screen for referral to a qualified test user such as a Clinical or Forensic Psychologist. Can be administered on paper or on a device such as a computer or tablet. Is free to use but is consistent with gold-standard measures. Is self-contained so that no additional information is required for it to be used, interpreted, and reported. Provides a report of the limitations of the tool to inform future development of the scheme. To assist in your task, you have been provided with a letter from the prison governor, Darren Johnson, providing more detail about HMP Birley and its rehabilitation programme. Please familiarise yourself with the assignment brief and assignment video presentation on the Moodle page for more information about the requirements of the report. 12.2.2 Letter from the Governor Dear Psychologist, RE: Commission for a pilot screening tool The staff in our Assessment &amp; Interventions unit have informed me that the report that your colleague provided on the group rehabilitation treatment was very helpful and they have started implementing some of the suggestions. As we informed your colleague, the programme is delivered to eight men a minimum of six hours a week over six, and non-attendance is costly and disruptive. Now that we have a better idea of why some men do not attend, our concern is with ensuring the men at HMP Birley receive appropriate support to reduce risk and prepare them for release. However, we have limited time and financial resources. Having discussed with the forensic psychologist, we have decided that we should pilot screening the men on B wing prior to enrolment to the group rehabilitation programme. As such, I would like to commission you to produce a short, free-to-use screening tool that may be administered by our intervention facilitators rather than their senior colleagues. The tool should identify men who will likely have minimal engaging with the programme, and those that need additional assessment and referral to more senior staff. As you will understand, the men in HMP Birley have limited access to digital technology, and so the tool needs to be both printable, and deliverable on digital devices. Moreover, staff are not permitted to bring devices into the prison, and so the tool needs to provide all information for delivering, scoring, and interpreting the findings in a single document. Although our facilitators are currently all psychology graduates, it is possible that some of them have yet to gain experience in writing reports, so it is essential that you provide a sample report for them to follow. Finally, I would like you to prepare a separate document that highlights the limitations of the tool and some suggestions for improvements should the pilot be successful and rolled out to the wider prison population. The men who we will be screening have a history of violent and/or acquisitive offences and are diverse in terms of their life experiences, offending, abilities, and other characteristics. For instance, some of the men show signs of mild to moderate intellectual or learning impairment. In addition, some of the men are neurodivergent, such as having attention deficit hyperactivity disorder (ADHD) or autism spectrum disorder. You colleague informs me that they have already identified a possible set of tests. Please can you ensure that there are no more than four measures in the final version; that it does not exceed 3000 words; and that it is delivered no later than 25th May 2025 (unless there are mitigating factors). We are keen to work with you to help plan a way forward so that we can make our programmes more efficient. Kind regards, Darren Johnson Governor HMP Birley "],["misc.html", "Chapter 13 Miscellaneous 13.1 What Do You Mean by Sub-Heading Levels? 13.2 Boundary Conditions In-depth 13.3 What Even Are Raw Scores? 13.4 Can’t Seem to Register a Registered Report 13.5 *%$@#ng Percentiles!?", " Chapter 13 Miscellaneous 13.1 What Do You Mean by Sub-Heading Levels? Here are some examples of sub-heading levels. You can find other guides online, but anything more than 3 gets a bit confusing. Level 1 (Major Section, Introduction etc) Level 2 (Minor section, e.g. construct of interest) Level 3 (minor(er) section, e.g. validity evidence) 13.2 Boundary Conditions In-depth Remember that it is important to distinguish between a construct, and a test. In Classical Test Theory terms, General Learning Disability is a condition in which a persons True Ability score - which is directly reflective of the construct - is substantially below the average for the population. Diagnosis of this would include (but not be limited to) tests of intelligence. Diagnosis of General Learning Disability requires gold standard tests to be administered by a qualified professional, and scores below a cut off would be provided as just one source of evidence for the condition. Gold-standard tests are required because they provide more accurate estimates of a person’s True score (better testing conditions mean the person will perform closer to their potential; the test will have higher reliability, better items, a representative norm group, and therefore smaller standard error of measurement, meaning fewer likely values for the True Score). In this case, the test score reflects the construct more precisely than a score obtained using a free test in a loud environment. So using the same boundary condition for ICAR as e.g the WAIS-5 will lead to far more false positives and false negatives because it will not capture the construct as precisely. For more about this see chapters six (click for link to Chapter 6), and seven (click for link to Chapter 7), and six (click for link to Chapter 4) 13.3 What Even Are Raw Scores? Lots of people struggle with this terminology - so let’s do an example. Imagine we have a test that has 10 items, these can either be Yes (1) or No (0). We give this test to a norm group and calculate that the average score was 5 out of 10, with a standard deviation of 2. This average score is on the Raw scale. It’s raw because it has not been processed into a Standard Scale such as a z score, T score or an IQ score. The limits to this Raw scale are 0 (no answers correct) and 10 (all answers correct). We give the test to another person and they get 6 answers correct. This means their Raw score for this test is 6 out of 10. The Raw score does not tell us how they compare with the Norm group - it just tells us how many correct answers they got. Then think of a personality test. The maximum score is 5, the minimum is 1. The norm group average was 2.8, with a standard deviation of 1.4. This is the Raw mean and standard deviation for this personality test. The same person takes this test and gets a score of 2.3. This is their Raw score for the personality test. Once again it does not tell us how they compare with the norm group, it just tells us that they responded between disagree and neutral on average across the questions. Standard scores can tell us how much a persons score differs from the norm group. For the ability test we can see that their raw score of 6 is higher than then the raw score of the norm group of 5 - but the standard score also tells us how extreme this difference is. Their z score for this test would be 0.5, and their T score would be 55. This means that they are above average, but not by much (most people score between 40 and 60). I’ll let you try the calculations - and then try and converting their personality Raw score to a T score. 13.4 Can’t Seem to Register a Registered Report A registered report is a two-stage research article. At Stage 1 - the researchers describe in detail what they intend to do for their research project (like a Dissertation Protocol but harder). This plan gets peer reviewed and if it passes, the researchers are given an In-Principal Acceptance by the journal - meaning that if they complete the study as described, they will almost always get published. At Stage 2 - the study has been completed, analysed, and written up. This document will include the participant demographics, results, and discussion. You have access to both parts because the Stage 1 report will contain justifications for using their scales that were made prior to data collection. At Stage 2 you will have the relevant participant characteristics, as well as any critique of the scales that were made after data collection. 13.5 *%$@#ng Percentiles!? Note that percentiles are covered in more detail in Chapter 3 (Click for link to Chapter 3) and the Statistics Primer (Click for link to Statistics Primer interquartile ranges, Click for link to Statistics Primer z scores and percentiles). There are also videos on MMU Tube that cover how to create a Percentile Table (Click for link to Jamovi video) These are hugely un-intuitive and and people struggle with them every year! So don’t worry, you are not alone… In simple terms - we are just saying what percentage of the norm group scored below a particular score. Note that this is not the same as saying that a score is a percentage below the norm group. For example - imagine there are 20 people in a room and each has bought cookies to snack on. The plot above shows that the distribution of the number of cookies is bell-shaped. Most people have five cookies, and there is a spread around this. We can look at this data in Jamovi: Now imagine that I enter the room, and in my bag I have seven cookies. I will have more than some of the people in the room, and fewer than the rest of the people. The percentile that I would be in for the attribute number of cookies possessed, is the percentage of people who have fewer cookies than I. Lets look at an example in Jamovi. Select Exploration -&gt; Descriptives and place cookies in the variables pane: Under the Statistics dropdown box, de-select everything except the mean and standard deviation options to get a clear table. Next, select the Percentiles check-box. This will bring up three quartile values (25th, 50th, 75th percentiles). Although the number of cookies that I have is included in this table, it is not very accurate since the maximum number in the room is 9. We can include more percentile values by adding more numbers to the text box. Below I have gone from 10 to 100 in steps of 10. We can see that my number of cookies (7) puts me in the 80th percentile for the room, meaning that I have more cookies than 80% of the people in the room. We can also see that this is not the same as having 80% more cookies than the room. If we took the mean average of 5.55 cookies and work out the percentage (7/5.5)*100, then we get 127%, which means I have 27% more cookies than the room average. When writing your test manual, you need to make sure that anyone who uses your tool can work out the percentile that the test taker is in - and so you would need to include a table that is similar to the one above. It might be that there is not enough variation in your norm group scores to allow for unique percentiles for every score (e.g. in the above there are three percentiles associated with a score of 5). If this is the case, then remove the redundant percentiles from the text-box or adding additional numbers in like in the example below. This will make reporting the percentile scores much easier! "]]
